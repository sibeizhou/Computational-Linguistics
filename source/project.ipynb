{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pickle(item, directory):\n",
    "    \"\"\"import data\n",
    "    \"\"\"\n",
    "    pickle.dump(item, open(directory,\"wb\"))\n",
    "\n",
    "\n",
    "def load_from_pickle(directory):\n",
    "    \"\"\"load data\n",
    "    \"\"\"\n",
    "    return pickle.load(open(directory,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/allData.csv',lineterminator='\\n',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>label\\r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>Negative\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>airport starts as a brand new luxury plane is ...</td>\n",
       "      <td>Negative\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>Negative\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sorry everyone i know this is supposed to be a...</td>\n",
       "      <td>Negative\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>Negative\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                             review     label\\r\n",
       "0   1  story of a man who has unnatural feelings for ...  Negative\\r\n",
       "1   2  airport starts as a brand new luxury plane is ...  Negative\\r\n",
       "2   3  this film lacked something i couldnt put my fi...  Negative\\r\n",
       "3   4  sorry everyone i know this is supposed to be a...  Negative\\r\n",
       "4   5  when i was little my parents took me along to ...  Negative\\r"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Data preprocessing can be roughly divided into the following parts.\n",
    "\n",
    "1. Convert all letters to Ascii\n",
    "\n",
    "2. Convert all uppercase to lowercase; At the same time, keep only common punctuation marks\n",
    "\n",
    "3. Create word2idx, idx2word, word2count(the number of times each word appears), n_word(the total number of words).\n",
    "\n",
    "4. Convert these sentences into Tensor and use index for each word\n",
    "\n",
    "5. Fill the sentences so that each sentence is the same length, so that you can use batch for training\n",
    "\n",
    "6. Convert the label to one-hot format for the convenience of the final training (in Pytorch, it only needs to be converted to label)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word2index and index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocess 1\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"change to Ascii\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Data preprocess 2\n",
    "def normalizeString(s):\n",
    "    \"\"\"change to lower case and remove punctuation\n",
    "    \"\"\"\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    return s\n",
    "\n",
    "# Data preprocess 3\n",
    "class Lang():\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"SOS\",1:\"EOS\"}\n",
    "        self.n_words = 2\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words \n",
    "            self.word2count[word] = 1 \n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words = self.n_words + 1\n",
    "        else:\n",
    "            self.word2count[word] = self.word2count[word] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count word:130331\n"
     ]
    }
   ],
   "source": [
    "lang = Lang()\n",
    "for sentence_data in data[\"review\"].values.tolist():\n",
    "    sentence_data = normalizeString(sentence_data)\n",
    "    lang.addSentence(sentence_data)\n",
    "\n",
    "print(\"Count word:{}\".format(lang.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 87.84213030100744, 665645)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count = np.array(list(lang.word2count.values()))\n",
    "\n",
    "np.median(data_count), np.mean(data_count), np.max(data_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for _,count in lang.word2count.items():\n",
    "    if count < 50:\n",
    "        less_count = less_count + count\n",
    "    total_count = total_count + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count word:11102\n"
     ]
    }
   ],
   "source": [
    "# We set word need to be appear 50 times to be counted\n",
    "lang_process = Lang()\n",
    "\n",
    "for word,count in lang.word2count.items():\n",
    "    if count >= 50:\n",
    "        lang_process.word2index[word] = lang_process.n_words \n",
    "        lang_process.word2count[word] = count\n",
    "        lang_process.index2word[lang_process.n_words] = word\n",
    "        lang_process.n_words = lang_process.n_words + 1\n",
    "        \n",
    "print(\"Count word:{}\".format(lang_process.n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': 22899,\n",
       " 'of': 289290,\n",
       " 'a': 321814,\n",
       " 'man': 11057,\n",
       " 'who': 40338,\n",
       " 'has': 32979,\n",
       " 'unnatural': 97,\n",
       " 'feelings': 796,\n",
       " 'for': 87263,\n",
       " 'pig': 162,\n",
       " 'starts': 2298,\n",
       " 'out': 34095,\n",
       " 'with': 87164,\n",
       " 'opening': 1985,\n",
       " 'scene': 10876,\n",
       " 'that': 136735,\n",
       " 'is': 210700,\n",
       " 'terrific': 829,\n",
       " 'example': 2664,\n",
       " 'absurd': 551,\n",
       " 'comedy': 6500,\n",
       " 'formal': 53,\n",
       " 'orchestra': 115,\n",
       " 'audience': 4209,\n",
       " 'turned': 1862,\n",
       " 'into': 17834,\n",
       " 'an': 42916,\n",
       " 'insane': 509,\n",
       " 'violent': 1010,\n",
       " 'mob': 351,\n",
       " 'by': 44422,\n",
       " 'the': 665645,\n",
       " 'crazy': 1262,\n",
       " 'its': 49085,\n",
       " 'singers': 156,\n",
       " 'unfortunately': 2590,\n",
       " 'it': 155572,\n",
       " 'stays': 334,\n",
       " 'whole': 6116,\n",
       " 'time': 24901,\n",
       " 'no': 25073,\n",
       " 'general': 1490,\n",
       " 'narrative': 818,\n",
       " 'eventually': 1404,\n",
       " 'making': 5760,\n",
       " 'just': 34986,\n",
       " 'too': 15273,\n",
       " 'off': 12003,\n",
       " 'putting': 722,\n",
       " 'even': 24715,\n",
       " 'those': 9367,\n",
       " 'from': 40429,\n",
       " 'era': 1161,\n",
       " 'should': 9615,\n",
       " 'be': 53328,\n",
       " 'dialogue': 3053,\n",
       " 'would': 24213,\n",
       " 'make': 15871,\n",
       " 'shakespeare': 454,\n",
       " 'seem': 4222,\n",
       " 'easy': 1583,\n",
       " 'to': 267907,\n",
       " 'third': 1374,\n",
       " 'on': 67902,\n",
       " 'technical': 549,\n",
       " 'level': 1871,\n",
       " 'better': 11363,\n",
       " 'than': 19319,\n",
       " 'you': 60028,\n",
       " 'might': 5743,\n",
       " 'think': 14295,\n",
       " 'some': 31045,\n",
       " 'good': 29535,\n",
       " 'cinematography': 1926,\n",
       " 'future': 1654,\n",
       " 'great': 18030,\n",
       " 'stars': 3145,\n",
       " 'sally': 281,\n",
       " 'and': 322347,\n",
       " 'forrest': 77,\n",
       " 'can': 21986,\n",
       " 'seen': 13325,\n",
       " 'briefly': 283,\n",
       " 'airport': 225,\n",
       " 'as': 91464,\n",
       " 'brand': 263,\n",
       " 'new': 8087,\n",
       " 'luxury': 72,\n",
       " 'plane': 706,\n",
       " 'loaded': 171,\n",
       " 'up': 26304,\n",
       " 'valuable': 174,\n",
       " 'paintings': 132,\n",
       " 'such': 9968,\n",
       " 'rich': 1196,\n",
       " 'businessman': 147,\n",
       " 'philip': 264,\n",
       " 'stevens': 194,\n",
       " 'james': 2015,\n",
       " 'stewart': 674,\n",
       " 'flying': 655,\n",
       " 'them': 15928,\n",
       " 'bunch': 1595,\n",
       " 'his': 57445,\n",
       " 'estate': 217,\n",
       " 'in': 186365,\n",
       " 'preparation': 61,\n",
       " 'being': 13095,\n",
       " 'opened': 309,\n",
       " 'public': 1076,\n",
       " 'museum': 215,\n",
       " 'also': 17873,\n",
       " 'board': 582,\n",
       " 'daughter': 2151,\n",
       " 'julie': 455,\n",
       " 'kathleen': 75,\n",
       " 'her': 34659,\n",
       " 'son': 2466,\n",
       " 'takes': 4258,\n",
       " 'planned': 220,\n",
       " 'but': 82201,\n",
       " 'mid': 618,\n",
       " 'air': 1325,\n",
       " 'hi': 167,\n",
       " 'co': 1113,\n",
       " 'pilot': 547,\n",
       " 'robert': 1983,\n",
       " 'two': 13503,\n",
       " 'wilson': 363,\n",
       " 'michael': 2440,\n",
       " 'knock': 239,\n",
       " 'passengers': 159,\n",
       " 'crew': 1324,\n",
       " 'sleeping': 335,\n",
       " 'gas': 349,\n",
       " 'they': 41658,\n",
       " 'plan': 834,\n",
       " 'steal': 489,\n",
       " 'cargo': 59,\n",
       " 'land': 809,\n",
       " 'strip': 260,\n",
       " 'isolated': 220,\n",
       " 'island': 1268,\n",
       " 'while': 10307,\n",
       " 'descent': 176,\n",
       " 'almost': 6233,\n",
       " 'hits': 547,\n",
       " 'oil': 268,\n",
       " 'ocean': 247,\n",
       " 'loses': 459,\n",
       " 'control': 996,\n",
       " 'sending': 145,\n",
       " 'crashing': 112,\n",
       " 'sea': 658,\n",
       " 'where': 12468,\n",
       " 'sinks': 91,\n",
       " 'bottom': 845,\n",
       " 'right': 6479,\n",
       " 'bang': 195,\n",
       " 'middle': 1877,\n",
       " 'triangle': 163,\n",
       " 'short': 3819,\n",
       " 'supply': 137,\n",
       " 'water': 1208,\n",
       " 'having': 4916,\n",
       " 'over': 12169,\n",
       " 'miles': 501,\n",
       " 'course': 4855,\n",
       " 'problems': 1764,\n",
       " 'mount': 55,\n",
       " 'survivors': 194,\n",
       " 'help': 3685,\n",
       " 'fast': 1688,\n",
       " 'running': 1926,\n",
       " 'known': 2223,\n",
       " 'under': 2699,\n",
       " 'slightly': 1068,\n",
       " 'different': 4648,\n",
       " 'this': 150105,\n",
       " 'second': 3819,\n",
       " 'sequel': 1594,\n",
       " 'smash': 102,\n",
       " 'hit': 2097,\n",
       " 'disaster': 678,\n",
       " 'thriller': 1727,\n",
       " 'was': 95472,\n",
       " 'directed': 2377,\n",
       " 'jerry': 605,\n",
       " 'once': 4575,\n",
       " 'again': 7765,\n",
       " 'like': 40003,\n",
       " 'predecessors': 77,\n",
       " 'i': 152597,\n",
       " 'cant': 7493,\n",
       " 'say': 10709,\n",
       " 'any': 15029,\n",
       " 'sort': 2925,\n",
       " 'forgotten': 706,\n",
       " 'classic': 3551,\n",
       " 'entertaining': 2911,\n",
       " 'although': 4937,\n",
       " 'not': 60434,\n",
       " 'necessarily': 352,\n",
       " 'reasons': 1183,\n",
       " 'three': 4664,\n",
       " 'films': 15618,\n",
       " 'have': 55162,\n",
       " 'so': 40547,\n",
       " 'far': 5976,\n",
       " 'actually': 8441,\n",
       " 'liked': 2938,\n",
       " 'one': 52798,\n",
       " 'best': 12561,\n",
       " 'my': 24801,\n",
       " 'favourite': 627,\n",
       " 'plot': 12881,\n",
       " 'nice': 3846,\n",
       " 'then': 15842,\n",
       " 'didnt': 8816,\n",
       " 'he': 52219,\n",
       " 'see': 22930,\n",
       " 'sinking': 115,\n",
       " 'maybe': 4636,\n",
       " 'makers': 943,\n",
       " 'were': 22284,\n",
       " 'trying': 4896,\n",
       " 'cross': 665,\n",
       " 'original': 6286,\n",
       " 'another': 8539,\n",
       " 'popular': 1055,\n",
       " 'flick': 2484,\n",
       " 'period': 1454,\n",
       " 'adventure': 975,\n",
       " 'until': 3467,\n",
       " 'end': 11033,\n",
       " 'stark': 177,\n",
       " 'dilemma': 116,\n",
       " 'facing': 179,\n",
       " 'trapped': 391,\n",
       " 'inside': 1279,\n",
       " 'either': 3625,\n",
       " 'when': 27931,\n",
       " 'runs': 937,\n",
       " 'or': 35455,\n",
       " 'if': 33659,\n",
       " 'doors': 251,\n",
       " 'are': 58314,\n",
       " 'decent': 2267,\n",
       " 'idea': 4052,\n",
       " 'could': 15197,\n",
       " 'made': 16099,\n",
       " 'little': 12418,\n",
       " 'bad': 18327,\n",
       " 'unsympathetic': 100,\n",
       " 'characters': 14954,\n",
       " 'dull': 1587,\n",
       " 'set': 4788,\n",
       " 'pieces': 874,\n",
       " 'real': 9404,\n",
       " 'lack': 2106,\n",
       " 'danger': 402,\n",
       " 'suspense': 1568,\n",
       " 'tension': 1041,\n",
       " 'means': 1493,\n",
       " 'missed': 1096,\n",
       " 'opportunity': 757,\n",
       " 'rather': 5265,\n",
       " 'keeps': 1262,\n",
       " 'entertained': 430,\n",
       " 'odd': 1133,\n",
       " 'minutes': 5858,\n",
       " 'much': 19245,\n",
       " 'happens': 2204,\n",
       " 'after': 14893,\n",
       " 'theres': 6086,\n",
       " 'urgency': 55,\n",
       " 'thought': 6920,\n",
       " 'there': 30979,\n",
       " 'been': 18341,\n",
       " 'navy': 278,\n",
       " 'become': 2939,\n",
       " 'involved': 2110,\n",
       " 'things': 7338,\n",
       " 'dont': 16935,\n",
       " 'pick': 956,\n",
       " 'few': 7945,\n",
       " 'shots': 1901,\n",
       " 'huge': 1916,\n",
       " 'ships': 218,\n",
       " 'helicopters': 54,\n",
       " 'about': 34102,\n",
       " 'something': 10055,\n",
       " 'lacking': 549,\n",
       " 'here': 10478,\n",
       " 'george': 1617,\n",
       " 'kennedy': 275,\n",
       " 'worker': 229,\n",
       " 'joe': 1191,\n",
       " 'back': 9618,\n",
       " 'only': 23172,\n",
       " 'gets': 6279,\n",
       " 'couple': 3416,\n",
       " 'scenes': 10463,\n",
       " 'barely': 956,\n",
       " 'says': 2250,\n",
       " 'anything': 5733,\n",
       " 'look': 8256,\n",
       " 'worried': 231,\n",
       " 'background': 1245,\n",
       " 'home': 3690,\n",
       " 'video': 3428,\n",
       " 'theatrical': 434,\n",
       " 'version': 4081,\n",
       " 'run': 2483,\n",
       " 'us': 7814,\n",
       " 'tv': 5825,\n",
       " 'versions': 420,\n",
       " 'add': 1665,\n",
       " 'extra': 674,\n",
       " 'hour': 2345,\n",
       " 'footage': 1336,\n",
       " 'including': 2056,\n",
       " 'credits': 1340,\n",
       " 'sequence': 1712,\n",
       " 'many': 13413,\n",
       " 'more': 27947,\n",
       " 'flashbacks': 488,\n",
       " 'flesh': 486,\n",
       " 'longer': 983,\n",
       " 'rescue': 470,\n",
       " 'discovery': 268,\n",
       " 'dead': 3660,\n",
       " 'bodies': 460,\n",
       " 'am': 5800,\n",
       " 'sure': 5260,\n",
       " 'sit': 1429,\n",
       " 'through': 9663,\n",
       " 'near': 1649,\n",
       " 'cut': 2053,\n",
       " 'expected': 1395,\n",
       " 'film': 77352,\n",
       " 'dated': 501,\n",
       " 'badly': 1275,\n",
       " 'horrible': 2500,\n",
       " 'fashions': 71,\n",
       " 'interior': 113,\n",
       " 'design': 617,\n",
       " 'choices': 352,\n",
       " 'will': 18069,\n",
       " 'other': 18086,\n",
       " 'toy': 312,\n",
       " 'model': 463,\n",
       " 'effects': 4481,\n",
       " 'arent': 1725,\n",
       " 'along': 3734,\n",
       " 'sequels': 443,\n",
       " 'pride': 261,\n",
       " 'place': 4685,\n",
       " 'awards': 436,\n",
       " 'hall': 456,\n",
       " 'shame': 1370,\n",
       " 'lots': 1641,\n",
       " 'worse': 2911,\n",
       " 'reckon': 53,\n",
       " 'thats': 7317,\n",
       " 'harsh': 394,\n",
       " 'action': 6465,\n",
       " 'pace': 1093,\n",
       " 'slow': 2172,\n",
       " 'excitement': 405,\n",
       " 'generated': 172,\n",
       " 'which': 23151,\n",
       " 'pretty': 7228,\n",
       " 'properly': 327,\n",
       " 'production': 3567,\n",
       " 'values': 945,\n",
       " 'alright': 369,\n",
       " 'nothing': 8289,\n",
       " 'spectacular': 495,\n",
       " 'acting': 12757,\n",
       " 'isnt': 6309,\n",
       " 'oscar': 1505,\n",
       " 'winner': 418,\n",
       " 'jack': 1766,\n",
       " 'lemmon': 161,\n",
       " 'said': 4278,\n",
       " 'since': 5704,\n",
       " 'mistake': 780,\n",
       " 'star': 3969,\n",
       " 'looks': 4512,\n",
       " 'old': 8593,\n",
       " 'frail': 52,\n",
       " 'lee': 1217,\n",
       " 'grant': 370,\n",
       " 'drunk': 527,\n",
       " 'sir': 381,\n",
       " 'christopher': 735,\n",
       " 'given': 3562,\n",
       " 'do': 18136,\n",
       " 'plenty': 1216,\n",
       " 'familiar': 1070,\n",
       " 'faces': 746,\n",
       " 'most': 17324,\n",
       " 'ideas': 1149,\n",
       " 'behind': 2400,\n",
       " 'bit': 5952,\n",
       " 'silly': 1828,\n",
       " 'bland': 532,\n",
       " 'direction': 2700,\n",
       " 'doesnt': 8874,\n",
       " 'though': 8631,\n",
       " 'shouldnt': 683,\n",
       " 'boring': 3593,\n",
       " 'followed': 687,\n",
       " 'concorde': 62,\n",
       " 'lacked': 275,\n",
       " 'couldnt': 3035,\n",
       " 'put': 4742,\n",
       " 'finger': 234,\n",
       " 'at': 46671,\n",
       " 'first': 17527,\n",
       " 'charisma': 254,\n",
       " 'part': 7846,\n",
       " 'leading': 1191,\n",
       " 'actress': 2301,\n",
       " 'inevitably': 150,\n",
       " 'translated': 127,\n",
       " 'chemistry': 923,\n",
       " 'she': 24043,\n",
       " 'shared': 141,\n",
       " 'screen': 4987,\n",
       " 'romantic': 1632,\n",
       " 'came': 3308,\n",
       " 'across': 1978,\n",
       " 'merely': 683,\n",
       " 'actors': 9034,\n",
       " 'play': 4477,\n",
       " 'very': 27656,\n",
       " 'well': 21021,\n",
       " 'director': 8195,\n",
       " 'what': 30448,\n",
       " 'needed': 1350,\n",
       " 'know': 12443,\n",
       " 'screenplay': 1279,\n",
       " 'exactly': 1960,\n",
       " 'chef': 74,\n",
       " 'love': 12906,\n",
       " 'seemed': 2703,\n",
       " 'skills': 506,\n",
       " 'restaurant': 253,\n",
       " 'ultimately': 972,\n",
       " 'himself': 4160,\n",
       " 'youthful': 109,\n",
       " 'exploits': 108,\n",
       " 'anybody': 588,\n",
       " 'else': 3838,\n",
       " 'never': 12957,\n",
       " 'convinced': 360,\n",
       " 'me': 21308,\n",
       " 'princess': 350,\n",
       " 'disappointed': 1801,\n",
       " 'movie': 86433,\n",
       " 'forget': 1451,\n",
       " 'nominated': 442,\n",
       " 'judge': 613,\n",
       " 'yourself': 1919,\n",
       " 'sorry': 1562,\n",
       " 'everyone': 4259,\n",
       " 'supposed': 2855,\n",
       " 'art': 2457,\n",
       " 'wow': 784,\n",
       " 'handed': 354,\n",
       " 'guns': 526,\n",
       " 'screening': 356,\n",
       " 'people': 17775,\n",
       " 'blow': 434,\n",
       " 'their': 22719,\n",
       " 'brains': 235,\n",
       " 'watch': 13872,\n",
       " 'excellent': 4078,\n",
       " 'painful': 806,\n",
       " 'absence': 214,\n",
       " 'sound': 2817,\n",
       " 'track': 821,\n",
       " 'brutal': 583,\n",
       " 'long': 6843,\n",
       " 'how': 17517,\n",
       " 'sitting': 911,\n",
       " 'talking': 1823,\n",
       " 'especially': 4899,\n",
       " 'complaining': 165,\n",
       " 'really': 23037,\n",
       " 'had': 22044,\n",
       " 'hard': 5260,\n",
       " 'getting': 3310,\n",
       " 'performances': 3475,\n",
       " 'dark': 2757,\n",
       " 'uninspired': 271,\n",
       " 'stuff': 2381,\n",
       " 'take': 6970,\n",
       " 'thing': 9071,\n",
       " 'maureen': 64,\n",
       " 'red': 1477,\n",
       " 'dress': 363,\n",
       " 'dancing': 1038,\n",
       " 'otherwise': 1310,\n",
       " 'ripoff': 91,\n",
       " 'bergman': 230,\n",
       " 'im': 9461,\n",
       " 'fan': 3885,\n",
       " 'f': 906,\n",
       " 'anyone': 5226,\n",
       " 'enjoyed': 2412,\n",
       " 'hours': 1948,\n",
       " 'lying': 291,\n",
       " 'parents': 1593,\n",
       " 'took': 2230,\n",
       " 'theater': 1569,\n",
       " 'interiors': 67,\n",
       " 'movies': 15972,\n",
       " 'watched': 4493,\n",
       " 'we': 19164,\n",
       " 'walked': 438,\n",
       " 'recently': 1144,\n",
       " 'lived': 774,\n",
       " 'rest': 3471,\n",
       " 'life': 12694,\n",
       " 'without': 6409,\n",
       " 'pretentious': 481,\n",
       " 'ponderous': 52,\n",
       " 'painfully': 423,\n",
       " 'piece': 3040,\n",
       " 's': 10453,\n",
       " 'wine': 209,\n",
       " 'cheese': 336,\n",
       " 'tripe': 181,\n",
       " 'woody': 312,\n",
       " 'allen': 496,\n",
       " 'favorite': 2382,\n",
       " 'directors': 1796,\n",
       " 'worst': 5317,\n",
       " 'crap': 2036,\n",
       " 'career': 1943,\n",
       " 'style': 3182,\n",
       " 'gives': 3066,\n",
       " 'muted': 68,\n",
       " 'insight': 369,\n",
       " 'lives': 2674,\n",
       " 'family': 5695,\n",
       " 'psychological': 515,\n",
       " 'damage': 210,\n",
       " 'caused': 437,\n",
       " 'divorce': 196,\n",
       " 'non': 1796,\n",
       " 'whatever': 1377,\n",
       " 'intentionally': 164,\n",
       " 'comic': 1680,\n",
       " 'relief': 473,\n",
       " 'music': 6399,\n",
       " 'shadowy': 56,\n",
       " 'pathos': 109,\n",
       " 'defined': 185,\n",
       " 'nature': 1297,\n",
       " 'using': 1533,\n",
       " 'method': 204,\n",
       " 'pronounced': 62,\n",
       " 'depth': 1010,\n",
       " 'meaning': 975,\n",
       " 'truth': 1375,\n",
       " 'beyond': 1840,\n",
       " 'simply': 3845,\n",
       " 'connection': 544,\n",
       " 'sympathy': 410,\n",
       " 'instead': 4277,\n",
       " 'felt': 2874,\n",
       " 'contempt': 101,\n",
       " 'parade': 132,\n",
       " 'whining': 101,\n",
       " 'quest': 325,\n",
       " 'identity': 484,\n",
       " 'amid': 75,\n",
       " 'backdrop': 248,\n",
       " 'baked': 89,\n",
       " 'fart': 84,\n",
       " 'room': 1834,\n",
       " 'speaks': 398,\n",
       " 'affected': 228,\n",
       " 'language': 1055,\n",
       " 'between': 6573,\n",
       " 'cigarettes': 59,\n",
       " 'lost': 2937,\n",
       " 'struggling': 340,\n",
       " 'desperate': 570,\n",
       " 'find': 8205,\n",
       " 'understanding': 592,\n",
       " 'goes': 4741,\n",
       " 'point': 6146,\n",
       " 'want': 7288,\n",
       " 'slap': 215,\n",
       " 'all': 46643,\n",
       " 'resolution': 244,\n",
       " 'drama': 2786,\n",
       " 'taken': 2023,\n",
       " 'extreme': 713,\n",
       " 'audiences': 1007,\n",
       " 'ability': 914,\n",
       " 'connect': 232,\n",
       " 'chose': 386,\n",
       " 'immersed': 52,\n",
       " 'themselves': 2309,\n",
       " 'feel': 5822,\n",
       " 'left': 4177,\n",
       " 'reason': 4506,\n",
       " 'found': 5155,\n",
       " 'self': 2270,\n",
       " 'indulgent': 168,\n",
       " 'going': 8180,\n",
       " 'promoting': 71,\n",
       " 'message': 1614,\n",
       " 'distorted': 75,\n",
       " 'techniques': 291,\n",
       " 'past': 2413,\n",
       " 'relevance': 107,\n",
       " 'highly': 2262,\n",
       " 'recommend': 3402,\n",
       " 'youre': 3815,\n",
       " 'feeling': 2199,\n",
       " 'happy': 1834,\n",
       " 'need': 3542,\n",
       " 'remind': 285,\n",
       " 'death': 3889,\n",
       " 'lets': 1981,\n",
       " 'pretend': 218,\n",
       " 'happened': 2040,\n",
       " 'appears': 1644,\n",
       " 'critics': 798,\n",
       " 'wooden': 650,\n",
       " 'kid': 2225,\n",
       " 'ourselves': 288,\n",
       " 'mostly': 1843,\n",
       " 'supportive': 76,\n",
       " 'allens': 90,\n",
       " 'pretensions': 51,\n",
       " 'accusations': 50,\n",
       " 'contrary': 233,\n",
       " 'notwithstanding': 65,\n",
       " 'get': 18334,\n",
       " 'why': 10383,\n",
       " 'generally': 857,\n",
       " 'applauded': 50,\n",
       " 'originality': 345,\n",
       " 'imitating': 64,\n",
       " 'brian': 562,\n",
       " 'ripping': 134,\n",
       " 'hitchcock': 548,\n",
       " 'horror': 7207,\n",
       " 'robin': 533,\n",
       " 'woods': 754,\n",
       " 'view': 1924,\n",
       " 'strange': 1796,\n",
       " 'form': 1474,\n",
       " 'cultural': 348,\n",
       " 'agree': 1138,\n",
       " 'attempt': 2035,\n",
       " 'york': 1372,\n",
       " 'intellectual': 348,\n",
       " 'less': 3759,\n",
       " 'years': 8883,\n",
       " 'swedish': 238,\n",
       " 'susan': 332,\n",
       " 'brother': 1942,\n",
       " 'carl': 234,\n",
       " 'sweden': 92,\n",
       " 'results': 512,\n",
       " 'wilde': 52,\n",
       " 'reference': 328,\n",
       " 'dickens': 159,\n",
       " 'curiosity': 256,\n",
       " 'shop': 542,\n",
       " 'heart': 2528,\n",
       " 'stone': 549,\n",
       " 'laugh': 2954,\n",
       " 'loud': 826,\n",
       " 'same': 8082,\n",
       " 'chock': 67,\n",
       " 'full': 3545,\n",
       " 'afraid': 642,\n",
       " 'anger': 351,\n",
       " 'looking': 5037,\n",
       " 'distance': 234,\n",
       " 'becoming': 673,\n",
       " 'directorial': 236,\n",
       " 'use': 3571,\n",
       " 'polite': 63,\n",
       " 'term': 367,\n",
       " 'close': 2462,\n",
       " 'parody': 501,\n",
       " 'incredibly': 1249,\n",
       " 'keep': 3229,\n",
       " 'reminding': 95,\n",
       " 'brilliant': 2384,\n",
       " 'talented': 1132,\n",
       " 'read': 3780,\n",
       " 'poem': 138,\n",
       " 'yours': 107,\n",
       " 'day': 5160,\n",
       " 'oh': 2846,\n",
       " 'caring': 313,\n",
       " 'these': 10697,\n",
       " 'however': 6857,\n",
       " 'quite': 7263,\n",
       " 'hilarious': 2081,\n",
       " 'dialog': 1663,\n",
       " 'funny': 8663,\n",
       " 'earlier': 1246,\n",
       " 'hes': 5706,\n",
       " 'lines': 3075,\n",
       " 'straight': 1682,\n",
       " 'cast': 7360,\n",
       " 'poor': 3821,\n",
       " 'mary': 865,\n",
       " 'beth': 72,\n",
       " 'hurt': 756,\n",
       " 'copy': 1237,\n",
       " 'neurotic': 108,\n",
       " 'habits': 68,\n",
       " 'turning': 668,\n",
       " 'embarrassing': 443,\n",
       " 'kenneth': 160,\n",
       " 'branagh': 157,\n",
       " 'celebrity': 177,\n",
       " 'basic': 967,\n",
       " 'dysfunctional': 148,\n",
       " 'quietly': 146,\n",
       " 'mother': 2986,\n",
       " 'seems': 7068,\n",
       " 'lifted': 148,\n",
       " 'bergmans': 71,\n",
       " 'winter': 223,\n",
       " 'light': 1877,\n",
       " 'melodrama': 382,\n",
       " 'tricked': 73,\n",
       " 'lot': 8051,\n",
       " 'existential': 71,\n",
       " 'angst': 159,\n",
       " 'comes': 4742,\n",
       " 'visual': 1076,\n",
       " 'tricks': 267,\n",
       " 'scratching': 94,\n",
       " 'paper': 475,\n",
       " 'surf': 96,\n",
       " 'walking': 820,\n",
       " 'beach': 467,\n",
       " 'etc': 2293,\n",
       " 'later': 4251,\n",
       " 'serious': 2047,\n",
       " 'ill': 2423,\n",
       " 'rarely': 623,\n",
       " 'funnier': 355,\n",
       " 'blame': 597,\n",
       " 'writers': 1350,\n",
       " 'clueless': 153,\n",
       " 'paid': 709,\n",
       " 'angie': 61,\n",
       " 'charlie': 759,\n",
       " 'denise': 63,\n",
       " 'jon': 315,\n",
       " 'wasnt': 4498,\n",
       " 'enough': 6862,\n",
       " 'hung': 168,\n",
       " 'each': 5190,\n",
       " 'got': 6939,\n",
       " 'r': 707,\n",
       " 'rating': 1825,\n",
       " 'anyway': 2170,\n",
       " 'bubble': 99,\n",
       " 'bath': 170,\n",
       " 'shot': 4088,\n",
       " 'year': 4279,\n",
       " 'woman': 5034,\n",
       " 'does': 11575,\n",
       " 'sheen': 179,\n",
       " 'potentially': 194,\n",
       " 'hot': 1377,\n",
       " 'relationships': 720,\n",
       " 'beautiful': 4190,\n",
       " 'sexy': 892,\n",
       " 'actresses': 708,\n",
       " 'world': 7189,\n",
       " 'laughs': 1281,\n",
       " 'whoopi': 90,\n",
       " 'goldberg': 129,\n",
       " 'judy': 159,\n",
       " 'predictable': 1666,\n",
       " 'surprised': 1593,\n",
       " 'five': 1723,\n",
       " 'waste': 2787,\n",
       " 'viewers': 1633,\n",
       " 'mediocre': 688,\n",
       " 'bag': 318,\n",
       " 'bitchy': 84,\n",
       " 'demeanor': 73,\n",
       " 'law': 990,\n",
       " 'order': 1920,\n",
       " 'carries': 344,\n",
       " 'failed': 920,\n",
       " 'come': 6264,\n",
       " 'anti': 1046,\n",
       " 'positive': 972,\n",
       " 'mess': 1235,\n",
       " 'marriage': 723,\n",
       " 'hopefully': 444,\n",
       " 'effort': 1543,\n",
       " 'produces': 95,\n",
       " 'way': 15548,\n",
       " 'weak': 1417,\n",
       " 'outdated': 108,\n",
       " 'country': 1818,\n",
       " 'walker': 298,\n",
       " 'care': 2737,\n",
       " 'watching': 9128,\n",
       " 'subject': 1504,\n",
       " 'believable': 1457,\n",
       " 'dc': 97,\n",
       " 'true': 4497,\n",
       " 'applause': 82,\n",
       " 'low': 3564,\n",
       " 'quick': 694,\n",
       " 'stay': 1547,\n",
       " 'q': 172,\n",
       " 'schrader': 53,\n",
       " 'ahead': 757,\n",
       " 'finished': 554,\n",
       " 'finish': 796,\n",
       " 'jumped': 157,\n",
       " 'next': 3492,\n",
       " 'try': 3569,\n",
       " 'figure': 1428,\n",
       " 'guess': 2663,\n",
       " 'carter': 229,\n",
       " 'private': 525,\n",
       " 'boyfriend': 821,\n",
       " 'artistic': 659,\n",
       " 'male': 1319,\n",
       " 'bondage': 87,\n",
       " 'torture': 658,\n",
       " 'pictures': 866,\n",
       " 'iraq': 147,\n",
       " 'thinking': 2295,\n",
       " 'create': 1152,\n",
       " 'character': 13243,\n",
       " 'car': 2379,\n",
       " 'work': 8503,\n",
       " 'sense': 4569,\n",
       " 'played': 5129,\n",
       " 'did': 12540,\n",
       " 'may': 6583,\n",
       " 'twenty': 618,\n",
       " 'ago': 2009,\n",
       " 'likely': 872,\n",
       " 'recognized': 211,\n",
       " 'lauren': 138,\n",
       " 'lily': 137,\n",
       " 'kristin': 74,\n",
       " 'unless': 1312,\n",
       " 'giant': 830,\n",
       " 'load': 243,\n",
       " 'paul': 1426,\n",
       " 'utterly': 844,\n",
       " 'own': 6573,\n",
       " 'directing': 1203,\n",
       " 'during': 4302,\n",
       " 'process': 646,\n",
       " 'whom': 1285,\n",
       " 'plays': 4350,\n",
       " 'homosexual': 210,\n",
       " 'social': 1044,\n",
       " 'companion': 200,\n",
       " 'bored': 1033,\n",
       " 'wives': 172,\n",
       " 'washington': 395,\n",
       " 'elite': 127,\n",
       " 'dimensional': 509,\n",
       " 'magazine': 256,\n",
       " 'around': 7133,\n",
       " 'front': 1211,\n",
       " 'camera': 3576,\n",
       " 'stick': 911,\n",
       " 'southern': 367,\n",
       " 'accent': 938,\n",
       " 'rack': 64,\n",
       " 'beginning': 2765,\n",
       " 'every': 7954,\n",
       " 'line': 3660,\n",
       " 'delivers': 699,\n",
       " 'heat': 251,\n",
       " 'south': 919,\n",
       " 'still': 10798,\n",
       " 'him': 17472,\n",
       " 'ounce': 63,\n",
       " 'energy': 624,\n",
       " 'monotonous': 85,\n",
       " 'attempts': 1149,\n",
       " 'affect': 167,\n",
       " 'kind': 5530,\n",
       " 'east': 389,\n",
       " 'clumsy': 213,\n",
       " 'deliver': 654,\n",
       " 'written': 3101,\n",
       " 'incapable': 110,\n",
       " 'rolling': 359,\n",
       " 'spite': 348,\n",
       " 'fact': 6885,\n",
       " 'german': 1135,\n",
       " 'several': 2851,\n",
       " 'languages': 85,\n",
       " 'italian': 1115,\n",
       " 'someone': 4403,\n",
       " 'ya': 215,\n",
       " 'leads': 1448,\n",
       " 'tolerable': 117,\n",
       " 'moments': 3258,\n",
       " 'supporting': 1627,\n",
       " 'bacall': 94,\n",
       " 'scott': 992,\n",
       " 'thomas': 632,\n",
       " 'managed': 834,\n",
       " 'dignity': 213,\n",
       " 'ever': 11957,\n",
       " 'redeem': 130,\n",
       " 'endless': 467,\n",
       " 'series': 6513,\n",
       " 'flaws': 693,\n",
       " 'worth': 4665,\n",
       " 'your': 11480,\n",
       " 'called': 2906,\n",
       " 'noteworthy': 101,\n",
       " 'events': 1716,\n",
       " 'together': 4427,\n",
       " 'total': 1266,\n",
       " 'everything': 4771,\n",
       " 'mean': 3297,\n",
       " 'basically': 1784,\n",
       " 'fill': 468,\n",
       " 'disturbing': 909,\n",
       " 'troubled': 286,\n",
       " 'gritty': 378,\n",
       " 'expect': 2350,\n",
       " 'literally': 923,\n",
       " 'storyline': 1541,\n",
       " 'coming': 2187,\n",
       " 'rosario': 56,\n",
       " 'dawson': 120,\n",
       " 'completely': 3821,\n",
       " 'wasted': 1166,\n",
       " 'naked': 865,\n",
       " 'nc': 52,\n",
       " 'saw': 6330,\n",
       " 'throw': 815,\n",
       " 'away': 5462,\n",
       " 'youll': 2640,\n",
       " 'probably': 5594,\n",
       " 'interesting': 6151,\n",
       " 'dreams': 745,\n",
       " 'discussion': 232,\n",
       " 'hints': 179,\n",
       " 'corruption': 183,\n",
       " 'murder': 2061,\n",
       " 'power': 1868,\n",
       " 'related': 395,\n",
       " 'topics': 130,\n",
       " 'sometimes': 2294,\n",
       " 'realistic': 1503,\n",
       " 'nevertheless': 446,\n",
       " 'development': 1205,\n",
       " 'tea': 227,\n",
       " 'drinking': 342,\n",
       " 'ceremony': 76,\n",
       " 'visuals': 492,\n",
       " 'stunning': 771,\n",
       " 'ease': 193,\n",
       " 'eye': 1609,\n",
       " 'strain': 83,\n",
       " 'dinner': 348,\n",
       " 'before': 8464,\n",
       " 'bed': 880,\n",
       " 'shocking': 675,\n",
       " 'sitcom': 326,\n",
       " 'performance': 5537,\n",
       " 'fight': 2196,\n",
       " 'job': 4435,\n",
       " 'bearable': 83,\n",
       " 'watchable': 616,\n",
       " 'shows': 4899,\n",
       " 'investigation': 255,\n",
       " 'circle': 203,\n",
       " 'powerful': 1230,\n",
       " 'mens': 107,\n",
       " 'thrilling': 277,\n",
       " 'engaging': 572,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_process.word2count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to Tensor\n",
    "-Convert text to Tensor(Before you need to tensor, you need to normalise the words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the sentences in the data and convert them into tensor in order\n",
    "\n",
    "# The words in the sentence also need to be standardized when is converted here\n",
    "def convertWord2index(word):\n",
    "    if lang_process.word2index.get(word)==None:\n",
    "        # some word appear only a few times are set to 1\n",
    "        return 1\n",
    "    else:\n",
    "        return lang_process.word2index.get(word)\n",
    "    \n",
    "input_tensor = [[convertWord2index(s) for s in normalizeString(es).split(' ')]  for es in data[\"review\"].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[180,\n",
       "  463,\n",
       "  14,\n",
       "  601,\n",
       "  35,\n",
       "  5603,\n",
       "  80,\n",
       "  3837,\n",
       "  1176,\n",
       "  10,\n",
       "  213,\n",
       "  3,\n",
       "  33,\n",
       "  404,\n",
       "  278,\n",
       "  3018,\n",
       "  163,\n",
       "  207,\n",
       "  194,\n",
       "  208,\n",
       "  930,\n",
       "  83,\n",
       "  33,\n",
       "  972,\n",
       "  38,\n",
       "  5800,\n",
       "  248,\n",
       "  3546,\n",
       "  4954,\n",
       "  80,\n",
       "  421,\n",
       "  1584,\n",
       "  51,\n",
       "  9189,\n",
       "  122,\n",
       "  33,\n",
       "  342,\n",
       "  4909,\n",
       "  787,\n",
       "  107,\n",
       "  35,\n",
       "  589,\n",
       "  7642,\n",
       "  14,\n",
       "  35,\n",
       "  888,\n",
       "  4505,\n",
       "  80,\n",
       "  2938,\n",
       "  107,\n",
       "  35,\n",
       "  545,\n",
       "  6449,\n",
       "  1,\n",
       "  33,\n",
       "  163,\n",
       "  2,\n",
       "  3,\n",
       "  33,\n",
       "  685,\n",
       "  2116,\n",
       "  910,\n",
       "  32,\n",
       "  4,\n",
       "  2304,\n",
       "  3,\n",
       "  163,\n",
       "  2963,\n",
       "  1,\n",
       "  1,\n",
       "  203,\n",
       "  6757,\n",
       "  63,\n",
       "  2329,\n",
       "  307,\n",
       "  3,\n",
       "  33,\n",
       "  463,\n",
       "  194,\n",
       "  1558,\n",
       "  1036,\n",
       "  27,\n",
       "  197,\n",
       "  3719,\n",
       "  351,\n",
       "  69,\n",
       "  63,\n",
       "  196,\n",
       "  17,\n",
       "  180,\n",
       "  463,\n",
       "  18,\n",
       "  7483,\n",
       "  107,\n",
       "  80,\n",
       "  3,\n",
       "  1492,\n",
       "  38,\n",
       "  120,\n",
       "  358,\n",
       "  107,\n",
       "  442,\n",
       "  38,\n",
       "  782,\n",
       "  80,\n",
       "  782,\n",
       "  38,\n",
       "  440,\n",
       "  38,\n",
       "  995,\n",
       "  33,\n",
       "  404,\n",
       "  2406,\n",
       "  6443,\n",
       "  3,\n",
       "  528,\n",
       "  601,\n",
       "  3,\n",
       "  373,\n",
       "  82,\n",
       "  56,\n",
       "  3510,\n",
       "  32,\n",
       "  33,\n",
       "  2280,\n",
       "  404,\n",
       "  2964,\n",
       "  628,\n",
       "  2388,\n",
       "  9851,\n",
       "  180,\n",
       "  463,\n",
       "  3,\n",
       "  35,\n",
       "  871,\n",
       "  1,\n",
       "  7,\n",
       "  1454,\n",
       "  107,\n",
       "  460,\n",
       "  33,\n",
       "  685,\n",
       "  9,\n",
       "  33,\n",
       "  1204,\n",
       "  852,\n",
       "  2183,\n",
       "  1,\n",
       "  576,\n",
       "  243,\n",
       "  1216,\n",
       "  4114,\n",
       "  105,\n",
       "  439,\n",
       "  2496,\n",
       "  80,\n",
       "  17,\n",
       "  18,\n",
       "  213,\n",
       "  4908,\n",
       "  7796],\n",
       " [1285,\n",
       "  83,\n",
       "  180,\n",
       "  2,\n",
       "  986,\n",
       "  122,\n",
       "  215,\n",
       "  2541,\n",
       "  1115,\n",
       "  2184,\n",
       "  14,\n",
       "  947,\n",
       "  623,\n",
       "  1766,\n",
       "  2666,\n",
       "  403,\n",
       "  105,\n",
       "  623,\n",
       "  1582,\n",
       "  107,\n",
       "  448,\n",
       "  80,\n",
       "  1021,\n",
       "  4,\n",
       "  5,\n",
       "  33,\n",
       "  6391,\n",
       "  180,\n",
       "  41,\n",
       "  18,\n",
       "  856,\n",
       "  3442,\n",
       "  32,\n",
       "  2400,\n",
       "  9784,\n",
       "  28,\n",
       "  1242,\n",
       "  5,\n",
       "  14,\n",
       "  2030,\n",
       "  2295,\n",
       "  170,\n",
       "  6,\n",
       "  549,\n",
       "  28,\n",
       "  8123,\n",
       "  5598,\n",
       "  63,\n",
       "  1026,\n",
       "  549,\n",
       "  275,\n",
       "  7653,\n",
       "  105,\n",
       "  2625,\n",
       "  221,\n",
       "  120,\n",
       "  4,\n",
       "  4892,\n",
       "  4745,\n",
       "  4475,\n",
       "  33,\n",
       "  837,\n",
       "  7084,\n",
       "  3,\n",
       "  1606,\n",
       "  33,\n",
       "  1135,\n",
       "  947,\n",
       "  10731,\n",
       "  18,\n",
       "  856,\n",
       "  32,\n",
       "  10501,\n",
       "  145,\n",
       "  105,\n",
       "  304,\n",
       "  14,\n",
       "  33,\n",
       "  3711,\n",
       "  448,\n",
       "  1027,\n",
       "  248,\n",
       "  4,\n",
       "  1364,\n",
       "  1015,\n",
       "  80,\n",
       "  285,\n",
       "  6255,\n",
       "  770,\n",
       "  897,\n",
       "  63,\n",
       "  6531,\n",
       "  1,\n",
       "  221,\n",
       "  1,\n",
       "  9784,\n",
       "  440,\n",
       "  194,\n",
       "  458,\n",
       "  493,\n",
       "  1698,\n",
       "  1,\n",
       "  4346,\n",
       "  86,\n",
       "  33,\n",
       "  6095,\n",
       "  1948,\n",
       "  80,\n",
       "  576,\n",
       "  430,\n",
       "  55,\n",
       "  3,\n",
       "  1493,\n",
       "  9784,\n",
       "  63,\n",
       "  9547,\n",
       "  13,\n",
       "  3,\n",
       "  7780,\n",
       "  10,\n",
       "  105,\n",
       "  5209,\n",
       "  86,\n",
       "  8647,\n",
       "  63,\n",
       "  118,\n",
       "  5838,\n",
       "  3404,\n",
       "  122,\n",
       "  118,\n",
       "  2096,\n",
       "  65,\n",
       "  432,\n",
       "  18,\n",
       "  4,\n",
       "  1893,\n",
       "  33,\n",
       "  129,\n",
       "  1,\n",
       "  78,\n",
       "  3,\n",
       "  180,\n",
       "  463,\n",
       "  248,\n",
       "  33,\n",
       "  4560,\n",
       "  8702,\n",
       "  80,\n",
       "  33,\n",
       "  155,\n",
       "  1369,\n",
       "  998,\n",
       "  5623,\n",
       "  373,\n",
       "  55,\n",
       "  203,\n",
       "  56,\n",
       "  604,\n",
       "  10,\n",
       "  2029,\n",
       "  107,\n",
       "  260,\n",
       "  528,\n",
       "  245,\n",
       "  180,\n",
       "  463,\n",
       "  33,\n",
       "  463,\n",
       "  7,\n",
       "  73,\n",
       "  370,\n",
       "  4897,\n",
       "  80,\n",
       "  251,\n",
       "  208,\n",
       "  1888,\n",
       "  33,\n",
       "  956,\n",
       "  272,\n",
       "  63,\n",
       "  1,\n",
       "  33,\n",
       "  852,\n",
       "  786,\n",
       "  122,\n",
       "  38,\n",
       "  899,\n",
       "  1482,\n",
       "  33,\n",
       "  331,\n",
       "  304,\n",
       "  248,\n",
       "  6879,\n",
       "  80,\n",
       "  440,\n",
       "  2626,\n",
       "  80,\n",
       "  2634,\n",
       "  63,\n",
       "  2811,\n",
       "  7411,\n",
       "  6476,\n",
       "  180,\n",
       "  463,\n",
       "  3908,\n",
       "  33,\n",
       "  25,\n",
       "  827,\n",
       "  80,\n",
       "  33,\n",
       "  827,\n",
       "  3,\n",
       "  215,\n",
       "  129,\n",
       "  6344]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the last two sentences\n",
    "input_tensor[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this movie with all its complexity and subtlety makes for one of the most thought provoking short films i have ever seen the topics it addresses are ugly cynical and at times even macabre but the film remains beautiful in its language artful with its camera angles and gorgeous in its style skillfully recreating the short story of the same name written by a master of short stories tobias wolff not wishing to spoil anything of the movie i wont go into any details other than to say that this movie is magnificent in and of itself it takes pride in what it does and does it well it shows the most important memories of life all of which can be topped by the single most elusive feeling unexpected bliss this movie of its own volition has created in me the same feelings the main character tom noonan felt when words transformed his very existence and that is one impressive feat',\n",
       " 'ive seen this story before but my kids havent boy with troubled past joins military faces his past falls in love and becomes a man the mentor this time is played perfectly by kevin costner an ordinary man with common everyday problems who lives an extraordinary conviction to save lives after losing his team he takes a teaching position training the next generation of heroes the young troubled recruit is played by kutcher while his scenes with the local love interest are a tad stiff and dont generate enough heat to melt butter he compliments costner well i never really understood sela ward as the neglected wife and felt she should of wanted costner to quit out of concern for his safety as opposed to her selfish needs but her presence on screen is a pleasure the two unaccredited stars of this movie are the coast guard and the sea both powerful forces which should not be taken for granted in real life or this movie the movie has some slow spots and could have used the wasted minutes to strengthen the character relationships but it still works the rescue scenes are intense and well filmed and edited to provide maximum impact this movie earns the audience applause and the applause of my two sons']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"review\"].values.tolist()[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  This index corresponds to word (you can see that it is case insensitive - this is important)\n",
    "lang_process.index2word[192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Padding\n",
    "\n",
    "- Convert Tensor into Tensor of fixed length, take away the superfluous ones and replace them with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228.96754\n",
      "172.0\n",
      "2462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Check the average/mediam/maximam length of the sentences\n",
    "sentence_length = [len(t) for t in input_tensor]\n",
    "print(np.mean(sentence_length))\n",
    "print(np.median(sentence_length))\n",
    "print(np.max(sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_3080\\3599462440.py:6: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "bins = np.arange(0,1400,100) # Generating interval scale\n",
    "\n",
    "ax.hist(sentence_length,bins=bins)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_3080\\1315754175.py:6: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "bins = np.arange(0,1000,10) # Generating interval scale\n",
    "\n",
    "ax.hist(sentence_length,bins=bins)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image above, we can see that a sentence length of 1000 is a good value.\n",
    "This means that sentences longer than 1000 words are removed from the end of the section (cut-off), and sentences less than 1000 words are added to the end of the 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_sequences(x, max_len):\n",
    "    padded = np.zeros((max_len), dtype=np.int64)\n",
    "    if len(x) > max_len: \n",
    "        padded[:] = x[:max_len]\n",
    "    else: \n",
    "        padded[:len(x)] = x\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = [pad_sequences(x, 1000) for x in input_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 180,  463,   14,  601,   35, 5603,   80, 3837, 1176,   10,  213,\n",
       "           3,   33,  404,  278, 3018,  163,  207,  194,  208,  930,   83,\n",
       "          33,  972,   38, 5800,  248, 3546, 4954,   80,  421, 1584,   51,\n",
       "        9189,  122,   33,  342, 4909,  787,  107,   35,  589, 7642,   14,\n",
       "          35,  888, 4505,   80, 2938,  107,   35,  545, 6449,    1,   33,\n",
       "         163,    2,    3,   33,  685, 2116,  910,   32,    4, 2304,    3,\n",
       "         163, 2963,    1,    1,  203, 6757,   63, 2329,  307,    3,   33,\n",
       "         463,  194, 1558, 1036,   27,  197, 3719,  351,   69,   63,  196,\n",
       "          17,  180,  463,   18, 7483,  107,   80,    3, 1492,   38,  120,\n",
       "         358,  107,  442,   38,  782,   80,  782,   38,  440,   38,  995,\n",
       "          33,  404, 2406, 6443,    3,  528,  601,    3,  373,   82,   56,\n",
       "        3510,   32,   33, 2280,  404, 2964,  628, 2388, 9851,  180,  463,\n",
       "           3,   35,  871,    1,    7, 1454,  107,  460,   33,  685,    9,\n",
       "          33, 1204,  852, 2183,    1,  576,  243, 1216, 4114,  105,  439,\n",
       "        2496,   80,   17,   18,  213, 4908, 7796,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       dtype=int64),\n",
       " array([ 1285,    83,   180,     2,   986,   122,   215,  2541,  1115,\n",
       "         2184,    14,   947,   623,  1766,  2666,   403,   105,   623,\n",
       "         1582,   107,   448,    80,  1021,     4,     5,    33,  6391,\n",
       "          180,    41,    18,   856,  3442,    32,  2400,  9784,    28,\n",
       "         1242,     5,    14,  2030,  2295,   170,     6,   549,    28,\n",
       "         8123,  5598,    63,  1026,   549,   275,  7653,   105,  2625,\n",
       "          221,   120,     4,  4892,  4745,  4475,    33,   837,  7084,\n",
       "            3,  1606,    33,  1135,   947, 10731,    18,   856,    32,\n",
       "        10501,   145,   105,   304,    14,    33,  3711,   448,  1027,\n",
       "          248,     4,  1364,  1015,    80,   285,  6255,   770,   897,\n",
       "           63,  6531,     1,   221,     1,  9784,   440,   194,   458,\n",
       "          493,  1698,     1,  4346,    86,    33,  6095,  1948,    80,\n",
       "          576,   430,    55,     3,  1493,  9784,    63,  9547,    13,\n",
       "            3,  7780,    10,   105,  5209,    86,  8647,    63,   118,\n",
       "         5838,  3404,   122,   118,  2096,    65,   432,    18,     4,\n",
       "         1893,    33,   129,     1,    78,     3,   180,   463,   248,\n",
       "           33,  4560,  8702,    80,    33,   155,  1369,   998,  5623,\n",
       "          373,    55,   203,    56,   604,    10,  2029,   107,   260,\n",
       "          528,   245,   180,   463,    33,   463,     7,    73,   370,\n",
       "         4897,    80,   251,   208,  1888,    33,   956,   272,    63,\n",
       "            1,    33,   852,   786,   122,    38,   899,  1482,    33,\n",
       "          331,   304,   248,  6879,    80,   440,  2626,    80,  2634,\n",
       "           63,  2811,  7411,  6476,   180,   463,  3908,    33,    25,\n",
       "          827,    80,    33,   827,     3,   215,   129,  6344,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0], dtype=int64)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data after the population is complete\n",
    "\n",
    "input_tensor[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tranfer the label to the one hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative\\r', 'Positive\\r'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at all the categories\n",
    "\n",
    "\n",
    "data['label\\r'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2emotion = {0: 'Negative\\r', 1: 'Positive\\r'}\n",
    "emotion2index = {'Negative\\r' : 0, 'Positive\\r' : 1}\n",
    "target_tensor = [emotion2index.get(s) for s in data['label\\r'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a quick look at the sample\n",
    "\n",
    "\n",
    "target_tensor[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_pickle(input_tensor, './data/input_tensor.pkl')\n",
    "# convert_to_pickle(target_tensor, './data/target_tensor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the pre-processing of the Data set is all done. The next step is to create the Data Loader, which can be used to finally put into the entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "- Partitioning of data sets, training sets and test sets\n",
    "- Data set loading, using DataLoader to load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000, 25000, 25000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Division of data sets (half part is training set and remain half part is test set)\n",
    "END = int(len(input_tensor)*0.5)\n",
    "\n",
    "input_tensor_train = torch.from_numpy(np.array(input_tensor[:END]))\n",
    "target_tensor_train = torch.from_numpy(np.array(target_tensor[:END])).long()\n",
    "\n",
    "input_tensor_test = torch.from_numpy(np.array(input_tensor[END:]))\n",
    "target_tensor_test = torch.from_numpy(np.array(target_tensor[END:])).long()\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_test), len(target_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = load_from_pickle('./data/test_loader.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,j in test_loader:\n",
    " #   print(i.shape,j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_dataset = Data.TensorDataset(input_tensor_train, target_tensor_train) \n",
    "test_dataset = Data.TensorDataset(input_tensor_test, target_tensor_test) \n",
    "\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=MINIBATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2           # set multi-work num read data\n",
    ")\n",
    "\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=MINIBATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2           # set multi-work num read data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_pickle(train_loader, './data/train_loader.pkl')\n",
    "# convert_to_pickle(test_loader, './data/test_loader.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've created your dataload, and you're ready to start building your network and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Simple RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN\n",
    "class EmotionRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size):\n",
    "        super(EmotionRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.batch_sz = batch_sz\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.rnn = nn.RNN(self.embedding_dim, self.hidden_units, batch_first = True)\n",
    "        self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.batch_sz = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        assert torch.equal(output[:, -1, :], hidden.squeeze(0))\n",
    "        output = self.fc(output[:,-1,:])\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "vocab_inp_size = len(lang_process.word2index)\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2 \n",
    "\n",
    "model = EmotionRNN(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size).to(device)\n",
    "\n",
    "# Test data\n",
    "it = iter(train_loader)\n",
    "x, y = next(it)\n",
    "\n",
    "output = model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target, logit):\n",
    "    ''' Obtain accuracy for training round '''\n",
    "    target = torch.max(target, 1)[1] # convert from one-hot encoding to class indices\n",
    "    corrects = (logit == target).sum()\n",
    "    accuracy = 100.0 * corrects / len(logit)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46.8750, device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output,y.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(output, 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bi-directional GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-GRU\n",
    "class EmotionGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size, layers=2):\n",
    "        super(EmotionGRU, self).__init__()\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.batch_sz = batch_sz\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = layers\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim) \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units, num_layers=self.num_layers, batch_first = True, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(self.hidden_units*2, self.output_size)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # Using bidirectional RNN, so num_layer = num_layer*2\n",
    "        return torch.zeros((self.num_layers*2, self.batch_sz, self.hidden_units)).to(device)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.batch_sz = x.size(0)\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        self.hidden = self.init_hidden()\n",
    "        output, self.hidden = self.gru(x, self.hidden)\n",
    "        # print(output.shape)\n",
    "        output = output[:,-1,:]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        # print(output.shape)\n",
    "        # output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "vocab_inp_size = len(lang_process.word2index)\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2 \n",
    "layers = 3\n",
    "\n",
    "model = EmotionGRU(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size, layers).to(device)\n",
    "\n",
    "# Test data\n",
    "it = iter(train_loader)\n",
    "x, y = next(it)\n",
    "\n",
    "output = model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 64*2, we have 64 samples, every sample is the probablity if the  2 emotion\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.1875, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output,y.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(output, 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating the Bi-directional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM\n",
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size, layers=2):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.batch_sz = batch_sz\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = layers\n",
    "        \n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.gru = nn.LSTM(self.embedding_dim, self.hidden_units, num_layers=self.num_layers, batch_first = True, bidirectional=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(self.hidden_units*2, self.output_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros((self.num_layers*2, self.batch_sz, self.hidden_units)).to(device),\n",
    "               torch.zeros((self.num_layers*2, self.batch_sz, self.hidden_units)).to(device))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.batch_sz = x.size(0)\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        (self.hidden, self.cell_state) = self.init_hidden()\n",
    "        output, (self.hidden, self.cell_state) = self.gru(x, (self.hidden, self.cell_state))\n",
    "        # print(output.shape)\n",
    "        output = output[:,-1,:]\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        # print(output.shape)\n",
    "        # output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesing model\n",
    "vocab_inp_size = len(lang_process.word2index) + 2\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2 # 2  emotion\n",
    "layers = 3\n",
    "\n",
    "# tesing model\n",
    "model = EmotionLSTM(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size, layers).to(device)\n",
    "\n",
    "# testing data\n",
    "it = iter(train_loader)\n",
    "x, y = next(it)\n",
    "\n",
    "output = model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35.9375, device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output,y.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(output, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start, end):\n",
    "    time = end - start\n",
    "    mins = int(time / 60)\n",
    "    secs = int(time - (mins * 60))\n",
    "    \n",
    "    return mins, secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for LSTM\n",
    "\n",
    "- Define the loss function\n",
    "\n",
    "- Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 Epoch 1 Batch 0 Accuracy 53.1250. Loss 0.0109\n",
      "Layer 2 Epoch 1 Batch 100 Accuracy 49.5823. Loss 0.0109\n",
      "Layer 2 Epoch 1 Batch 200 Accuracy 49.3004. Loss 0.0109\n",
      "Layer 2 Epoch 1 Batch 300 Accuracy 49.8443. Loss 0.0109\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 1 Accuracy 50.0304\n",
      "Epoch time : 2mins 37secs\n",
      "============\n",
      "Layer 2 Epoch 2 Batch 0 Accuracy 50.0000. Loss 0.0109\n",
      "Layer 2 Epoch 2 Batch 100 Accuracy 50.1083. Loss 0.0108\n",
      "Layer 2 Epoch 2 Batch 200 Accuracy 50.3809. Loss 0.0108\n",
      "Layer 2 Epoch 2 Batch 300 Accuracy 50.0363. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 2 Accuracy 50.0184\n",
      "Epoch time : 2mins 37secs\n",
      "============\n",
      "Layer 2 Epoch 3 Batch 0 Accuracy 48.4375. Loss 0.0108\n",
      "Layer 2 Epoch 3 Batch 100 Accuracy 49.0563. Loss 0.0108\n",
      "Layer 2 Epoch 3 Batch 200 Accuracy 49.4092. Loss 0.0108\n",
      "Layer 2 Epoch 3 Batch 300 Accuracy 49.6107. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 3 Accuracy 49.9896\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 4 Batch 0 Accuracy 54.6875. Loss 0.0108\n",
      "Layer 2 Epoch 4 Batch 100 Accuracy 49.7525. Loss 0.0108\n",
      "Layer 2 Epoch 4 Batch 200 Accuracy 49.6813. Loss 0.0108\n",
      "Layer 2 Epoch 4 Batch 300 Accuracy 50.0311. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 4 Accuracy 50.0352\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 5 Batch 0 Accuracy 46.8750. Loss 0.0109\n",
      "Layer 2 Epoch 5 Batch 100 Accuracy 50.9746. Loss 0.0108\n",
      "Layer 2 Epoch 5 Batch 200 Accuracy 51.0261. Loss 0.0108\n",
      "Layer 2 Epoch 5 Batch 300 Accuracy 50.7942. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 5 Accuracy 49.9664\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 6 Batch 0 Accuracy 59.3750. Loss 0.0108\n",
      "Layer 2 Epoch 6 Batch 100 Accuracy 49.1027. Loss 0.0108\n",
      "Layer 2 Epoch 6 Batch 200 Accuracy 49.9378. Loss 0.0108\n",
      "Layer 2 Epoch 6 Batch 300 Accuracy 49.5951. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 6 Accuracy 49.9608\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 7 Batch 0 Accuracy 51.5625. Loss 0.0114\n",
      "Layer 2 Epoch 7 Batch 100 Accuracy 49.0563. Loss 0.0111\n",
      "Layer 2 Epoch 7 Batch 200 Accuracy 49.5647. Loss 0.0111\n",
      "Layer 2 Epoch 7 Batch 300 Accuracy 49.7197. Loss 0.0111\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 7 Accuracy 50.2781\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 8 Batch 0 Accuracy 46.8750. Loss 0.0109\n",
      "Layer 2 Epoch 8 Batch 100 Accuracy 50.4177. Loss 0.0110\n",
      "Layer 2 Epoch 8 Batch 200 Accuracy 50.4975. Loss 0.0109\n",
      "Layer 2 Epoch 8 Batch 300 Accuracy 50.4309. Loss 0.0109\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 8 Accuracy 49.9480\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 9 Batch 0 Accuracy 50.0000. Loss 0.0109\n",
      "Layer 2 Epoch 9 Batch 100 Accuracy 50.7735. Loss 0.0109\n",
      "Layer 2 Epoch 9 Batch 200 Accuracy 49.8989. Loss 0.0109\n",
      "Layer 2 Epoch 9 Batch 300 Accuracy 49.8391. Loss 0.0109\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 9 Accuracy 49.6947\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 10 Batch 0 Accuracy 40.6250. Loss 0.0124\n",
      "Layer 2 Epoch 10 Batch 100 Accuracy 49.5978. Loss 0.0111\n",
      "Layer 2 Epoch 10 Batch 200 Accuracy 49.9534. Loss 0.0111\n",
      "Layer 2 Epoch 10 Batch 300 Accuracy 50.0311. Loss 0.0110\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 10 Accuracy 50.1127\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 11 Batch 0 Accuracy 51.5625. Loss 0.0108\n",
      "Layer 2 Epoch 11 Batch 100 Accuracy 50.6343. Loss 0.0109\n",
      "Layer 2 Epoch 11 Batch 200 Accuracy 49.9922. Loss 0.0109\n",
      "Layer 2 Epoch 11 Batch 300 Accuracy 49.9792. Loss 0.0109\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 11 Accuracy 50.0192\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 12 Batch 0 Accuracy 45.3125. Loss 0.0111\n",
      "Layer 2 Epoch 12 Batch 100 Accuracy 50.5724. Loss 0.0108\n",
      "Layer 2 Epoch 12 Batch 200 Accuracy 50.5208. Loss 0.0108\n",
      "Layer 2 Epoch 12 Batch 300 Accuracy 50.1921. Loss 0.0109\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 12 Accuracy 49.9880\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 13 Batch 0 Accuracy 53.1250. Loss 0.0108\n",
      "Layer 2 Epoch 13 Batch 100 Accuracy 50.4177. Loss 0.0108\n",
      "Layer 2 Epoch 13 Batch 200 Accuracy 50.1399. Loss 0.0108\n",
      "Layer 2 Epoch 13 Batch 300 Accuracy 49.7716. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 13 Accuracy 50.0424\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 14 Batch 0 Accuracy 65.6250. Loss 0.0107\n",
      "Layer 2 Epoch 14 Batch 100 Accuracy 50.0000. Loss 0.0108\n",
      "Layer 2 Epoch 14 Batch 200 Accuracy 49.8678. Loss 0.0108\n",
      "Layer 2 Epoch 14 Batch 300 Accuracy 49.7664. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 14 Accuracy 49.9752\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 15 Batch 0 Accuracy 46.8750. Loss 0.0110\n",
      "Layer 2 Epoch 15 Batch 100 Accuracy 50.4641. Loss 0.0108\n",
      "Layer 2 Epoch 15 Batch 200 Accuracy 50.5675. Loss 0.0108\n",
      "Layer 2 Epoch 15 Batch 300 Accuracy 50.2128. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 15 Accuracy 49.9648\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 16 Batch 0 Accuracy 43.7500. Loss 0.0109\n",
      "Layer 2 Epoch 16 Batch 100 Accuracy 50.1702. Loss 0.0108\n",
      "Layer 2 Epoch 16 Batch 200 Accuracy 50.0855. Loss 0.0108\n",
      "Layer 2 Epoch 16 Batch 300 Accuracy 49.9844. Loss 0.0108\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 16 Accuracy 52.9835\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 17 Batch 0 Accuracy 54.6875. Loss 0.0107\n",
      "Layer 2 Epoch 17 Batch 100 Accuracy 52.4752. Loss 0.0108\n",
      "Layer 2 Epoch 17 Batch 200 Accuracy 54.3921. Loss 0.0107\n",
      "Layer 2 Epoch 17 Batch 300 Accuracy 56.3746. Loss 0.0106\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 17 Accuracy 62.0820\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 18 Batch 0 Accuracy 71.8750. Loss 0.0104\n",
      "Layer 2 Epoch 18 Batch 100 Accuracy 68.8274. Loss 0.0096\n",
      "Layer 2 Epoch 18 Batch 200 Accuracy 72.0149. Loss 0.0092\n",
      "Layer 2 Epoch 18 Batch 300 Accuracy 74.3978. Loss 0.0088\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 18 Accuracy 79.3230\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 19 Batch 0 Accuracy 85.9375. Loss 0.0069\n",
      "Layer 2 Epoch 19 Batch 100 Accuracy 82.5031. Loss 0.0072\n",
      "Layer 2 Epoch 19 Batch 200 Accuracy 79.9596. Loss 0.0076\n",
      "Layer 2 Epoch 19 Batch 300 Accuracy 80.2326. Loss 0.0075\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 19 Accuracy 79.2319\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 20 Batch 0 Accuracy 87.5000. Loss 0.0063\n",
      "Layer 2 Epoch 20 Batch 100 Accuracy 84.1894. Loss 0.0068\n",
      "Layer 2 Epoch 20 Batch 200 Accuracy 83.7842. Loss 0.0068\n",
      "Layer 2 Epoch 20 Batch 300 Accuracy 84.1310. Loss 0.0067\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 20 Accuracy 80.8720\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 21 Batch 0 Accuracy 85.9375. Loss 0.0057\n",
      "Layer 2 Epoch 21 Batch 100 Accuracy 86.4171. Loss 0.0060\n",
      "Layer 2 Epoch 21 Batch 200 Accuracy 86.8315. Loss 0.0059\n",
      "Layer 2 Epoch 21 Batch 300 Accuracy 86.2749. Loss 0.0060\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 21 Accuracy 82.9771\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 22 Batch 0 Accuracy 92.1875. Loss 0.0040\n",
      "Layer 2 Epoch 22 Batch 100 Accuracy 88.1033. Loss 0.0053\n",
      "Layer 2 Epoch 22 Batch 200 Accuracy 88.3706. Loss 0.0053\n",
      "Layer 2 Epoch 22 Batch 300 Accuracy 87.8011. Loss 0.0055\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 22 Accuracy 83.0211\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 23 Batch 0 Accuracy 84.3750. Loss 0.0067\n",
      "Layer 2 Epoch 23 Batch 100 Accuracy 89.2946. Loss 0.0049\n",
      "Layer 2 Epoch 23 Batch 200 Accuracy 88.7749. Loss 0.0050\n",
      "Layer 2 Epoch 23 Batch 300 Accuracy 88.8081. Loss 0.0051\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 23 Accuracy 83.9570\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 24 Batch 0 Accuracy 93.7500. Loss 0.0043\n",
      "Layer 2 Epoch 24 Batch 100 Accuracy 91.7234. Loss 0.0042\n",
      "Layer 2 Epoch 24 Batch 200 Accuracy 90.5162. Loss 0.0046\n",
      "Layer 2 Epoch 24 Batch 300 Accuracy 90.5783. Loss 0.0045\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 24 Accuracy 84.4333\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 25 Batch 0 Accuracy 93.7500. Loss 0.0030\n",
      "Layer 2 Epoch 25 Batch 100 Accuracy 93.1931. Loss 0.0035\n",
      "Layer 2 Epoch 25 Batch 200 Accuracy 92.6228. Loss 0.0037\n",
      "Layer 2 Epoch 25 Batch 300 Accuracy 92.7481. Loss 0.0037\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 25 Accuracy 85.0440\n",
      "Epoch time : 2mins 36secs\n",
      "============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 Epoch 26 Batch 0 Accuracy 98.4375. Loss 0.0016\n",
      "Layer 2 Epoch 26 Batch 100 Accuracy 94.4152. Loss 0.0030\n",
      "Layer 2 Epoch 26 Batch 200 Accuracy 92.6928. Loss 0.0035\n",
      "Layer 2 Epoch 26 Batch 300 Accuracy 92.7014. Loss 0.0035\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 26 Accuracy 84.9433\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 27 Batch 0 Accuracy 95.3125. Loss 0.0025\n",
      "Layer 2 Epoch 27 Batch 100 Accuracy 94.4616. Loss 0.0028\n",
      "Layer 2 Epoch 27 Batch 200 Accuracy 94.4419. Loss 0.0028\n",
      "Layer 2 Epoch 27 Batch 300 Accuracy 94.5235. Loss 0.0028\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 27 Accuracy 85.2502\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 28 Batch 0 Accuracy 98.4375. Loss 0.0012\n",
      "Layer 2 Epoch 28 Batch 100 Accuracy 95.5136. Loss 0.0024\n",
      "Layer 2 Epoch 28 Batch 200 Accuracy 95.5924. Loss 0.0024\n",
      "Layer 2 Epoch 28 Batch 300 Accuracy 95.6188. Loss 0.0023\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 28 Accuracy 85.0416\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 29 Batch 0 Accuracy 96.8750. Loss 0.0010\n",
      "Layer 2 Epoch 29 Batch 100 Accuracy 96.4109. Loss 0.0019\n",
      "Layer 2 Epoch 29 Batch 200 Accuracy 96.2531. Loss 0.0020\n",
      "Layer 2 Epoch 29 Batch 300 Accuracy 96.2054. Loss 0.0020\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 29 Accuracy 85.5195\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 30 Batch 0 Accuracy 93.7500. Loss 0.0045\n",
      "Layer 2 Epoch 30 Batch 100 Accuracy 97.0606. Loss 0.0016\n",
      "Layer 2 Epoch 30 Batch 200 Accuracy 96.8517. Loss 0.0016\n",
      "Layer 2 Epoch 30 Batch 300 Accuracy 96.8802. Loss 0.0016\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 30 Accuracy 84.9888\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 31 Batch 0 Accuracy 98.4375. Loss 0.0005\n",
      "Layer 2 Epoch 31 Batch 100 Accuracy 97.4783. Loss 0.0012\n",
      "Layer 2 Epoch 31 Batch 200 Accuracy 97.1782. Loss 0.0015\n",
      "Layer 2 Epoch 31 Batch 300 Accuracy 96.5012. Loss 0.0017\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 31 Accuracy 83.4423\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 32 Batch 0 Accuracy 98.4375. Loss 0.0011\n",
      "Layer 2 Epoch 32 Batch 100 Accuracy 97.1380. Loss 0.0015\n",
      "Layer 2 Epoch 32 Batch 200 Accuracy 97.5202. Loss 0.0014\n",
      "Layer 2 Epoch 32 Batch 300 Accuracy 97.2020. Loss 0.0015\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 32 Accuracy 83.9898\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 33 Batch 0 Accuracy 100.0000. Loss 0.0006\n",
      "Layer 2 Epoch 33 Batch 100 Accuracy 98.1900. Loss 0.0011\n",
      "Layer 2 Epoch 33 Batch 200 Accuracy 97.7534. Loss 0.0012\n",
      "Layer 2 Epoch 33 Batch 300 Accuracy 97.8405. Loss 0.0012\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 33 Accuracy 84.5213\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 34 Batch 0 Accuracy 100.0000. Loss 0.0002\n",
      "Layer 2 Epoch 34 Batch 100 Accuracy 98.7005. Loss 0.0008\n",
      "Layer 2 Epoch 34 Batch 200 Accuracy 98.5774. Loss 0.0008\n",
      "Layer 2 Epoch 34 Batch 300 Accuracy 98.2558. Loss 0.0010\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 34 Accuracy 84.6731\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 35 Batch 0 Accuracy 100.0000. Loss 0.0004\n",
      "Layer 2 Epoch 35 Batch 100 Accuracy 98.6696. Loss 0.0008\n",
      "Layer 2 Epoch 35 Batch 200 Accuracy 98.4997. Loss 0.0009\n",
      "Layer 2 Epoch 35 Batch 300 Accuracy 98.2662. Loss 0.0010\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 35 Accuracy 84.3502\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 36 Batch 0 Accuracy 98.4375. Loss 0.0012\n",
      "Layer 2 Epoch 36 Batch 100 Accuracy 98.3292. Loss 0.0009\n",
      "Layer 2 Epoch 36 Batch 200 Accuracy 98.1343. Loss 0.0010\n",
      "Layer 2 Epoch 36 Batch 300 Accuracy 97.8146. Loss 0.0011\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 36 Accuracy 83.7212\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 37 Batch 0 Accuracy 96.8750. Loss 0.0013\n",
      "Layer 2 Epoch 37 Batch 100 Accuracy 97.0761. Loss 0.0015\n",
      "Layer 2 Epoch 37 Batch 200 Accuracy 97.4347. Loss 0.0014\n",
      "Layer 2 Epoch 37 Batch 300 Accuracy 97.3422. Loss 0.0014\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 37 Accuracy 84.5636\n",
      "Epoch time : 2mins 35secs\n",
      "============\n",
      "Layer 2 Epoch 38 Batch 0 Accuracy 98.4375. Loss 0.0015\n",
      "Layer 2 Epoch 38 Batch 100 Accuracy 98.7314. Loss 0.0008\n",
      "Layer 2 Epoch 38 Batch 200 Accuracy 98.7562. Loss 0.0008\n",
      "Layer 2 Epoch 38 Batch 300 Accuracy 98.6867. Loss 0.0008\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 38 Accuracy 84.4230\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 39 Batch 0 Accuracy 100.0000. Loss 0.0002\n",
      "Layer 2 Epoch 39 Batch 100 Accuracy 99.2265. Loss 0.0006\n",
      "Layer 2 Epoch 39 Batch 200 Accuracy 99.0283. Loss 0.0007\n",
      "Layer 2 Epoch 39 Batch 300 Accuracy 99.0189. Loss 0.0007\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 39 Accuracy 83.9642\n",
      "Epoch time : 2mins 37secs\n",
      "============\n",
      "Layer 2 Epoch 40 Batch 0 Accuracy 98.4375. Loss 0.0019\n",
      "Layer 2 Epoch 40 Batch 100 Accuracy 98.7624. Loss 0.0008\n",
      "Layer 2 Epoch 40 Batch 200 Accuracy 98.7873. Loss 0.0007\n",
      "Layer 2 Epoch 40 Batch 300 Accuracy 98.8113. Loss 0.0008\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 40 Accuracy 83.0011\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 41 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Layer 2 Epoch 41 Batch 100 Accuracy 98.3756. Loss 0.0009\n",
      "Layer 2 Epoch 41 Batch 200 Accuracy 98.5774. Loss 0.0008\n",
      "Layer 2 Epoch 41 Batch 300 Accuracy 98.6400. Loss 0.0008\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 41 Accuracy 83.6333\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 42 Batch 0 Accuracy 98.4375. Loss 0.0005\n",
      "Layer 2 Epoch 42 Batch 100 Accuracy 99.0099. Loss 0.0007\n",
      "Layer 2 Epoch 42 Batch 200 Accuracy 99.0594. Loss 0.0006\n",
      "Layer 2 Epoch 42 Batch 300 Accuracy 98.9151. Loss 0.0007\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 42 Accuracy 83.8211\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 43 Batch 0 Accuracy 98.4375. Loss 0.0013\n",
      "Layer 2 Epoch 43 Batch 100 Accuracy 98.9480. Loss 0.0007\n",
      "Layer 2 Epoch 43 Batch 200 Accuracy 98.9583. Loss 0.0006\n",
      "Layer 2 Epoch 43 Batch 300 Accuracy 98.6348. Loss 0.0008\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 43 Accuracy 84.2319\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 44 Batch 0 Accuracy 96.8750. Loss 0.0017\n",
      "Layer 2 Epoch 44 Batch 100 Accuracy 98.5767. Loss 0.0009\n",
      "Layer 2 Epoch 44 Batch 200 Accuracy 98.4764. Loss 0.0009\n",
      "Layer 2 Epoch 44 Batch 300 Accuracy 98.4998. Loss 0.0009\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 44 Accuracy 83.6101\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 45 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Layer 2 Epoch 45 Batch 100 Accuracy 98.7314. Loss 0.0008\n",
      "Layer 2 Epoch 45 Batch 200 Accuracy 99.0361. Loss 0.0007\n",
      "Layer 2 Epoch 45 Batch 300 Accuracy 99.0500. Loss 0.0007\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 45 Accuracy 84.4293\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 46 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Layer 2 Epoch 46 Batch 100 Accuracy 99.0873. Loss 0.0005\n",
      "Layer 2 Epoch 46 Batch 200 Accuracy 99.0205. Loss 0.0006\n",
      "Layer 2 Epoch 46 Batch 300 Accuracy 99.0293. Loss 0.0006\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 46 Accuracy 84.1416\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 47 Batch 0 Accuracy 100.0000. Loss 0.0002\n",
      "Layer 2 Epoch 47 Batch 100 Accuracy 99.4895. Loss 0.0004\n",
      "Layer 2 Epoch 47 Batch 200 Accuracy 99.4714. Loss 0.0004\n",
      "Layer 2 Epoch 47 Batch 300 Accuracy 99.4498. Loss 0.0004\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 47 Accuracy 83.7820\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 48 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Layer 2 Epoch 48 Batch 100 Accuracy 99.3502. Loss 0.0005\n",
      "Layer 2 Epoch 48 Batch 200 Accuracy 99.0983. Loss 0.0006\n",
      "Layer 2 Epoch 48 Batch 300 Accuracy 99.1694. Loss 0.0005\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 48 Accuracy 84.5133\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 49 Batch 0 Accuracy 98.4375. Loss 0.0012\n",
      "Layer 2 Epoch 49 Batch 100 Accuracy 98.9480. Loss 0.0007\n",
      "Layer 2 Epoch 49 Batch 200 Accuracy 99.0283. Loss 0.0006\n",
      "Layer 2 Epoch 49 Batch 300 Accuracy 98.9774. Loss 0.0006\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 49 Accuracy 84.7466\n",
      "Epoch time : 2mins 36secs\n",
      "============\n",
      "Layer 2 Epoch 50 Batch 0 Accuracy 100.0000. Loss 0.0002\n",
      "Layer 2 Epoch 50 Batch 100 Accuracy 99.1491. Loss 0.0005\n",
      "Layer 2 Epoch 50 Batch 200 Accuracy 99.1527. Loss 0.0005\n",
      "Layer 2 Epoch 50 Batch 300 Accuracy 99.2369. Loss 0.0005\n",
      "------------\n",
      "Test : Lay 2, Model LSTM, Epoch 50 Accuracy 84.5924\n",
      "Epoch time : 2mins 36secs\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameter\n",
    "vocab_inp_size = len(lang_process.word2index) + 2\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2\n",
    "num_layers = [2]\n",
    "\n",
    "for layers in num_layers:\n",
    "    # Test model\n",
    "    modelLSTM = EmotionLSTM(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size, layers).to(device)\n",
    "    models = {}\n",
    "    \n",
    "    models['LSTM'] = modelLSTM\n",
    "    # models['GRU'] = modelGRU\n",
    "    for key, model in models.items():\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "        # Start training\n",
    "        num_epochs = 50\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            start = time.time()\n",
    "\n",
    "            train_total_loss = 0 # Record the average loss in an entire epoch\n",
    "            train_total_accuracy = 0 # Record the average accuracy in an entire epoch\n",
    "\n",
    "            ### Training\n",
    "            for batch, (inp, targ) in enumerate(train_loader):\n",
    "                predictions = model(inp.to(device))\n",
    "                # Error in calculation   \n",
    "                loss = criterion(predictions, targ.to(device))\n",
    "                # Reverse propagation to modify the weight\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Record Loss decreases and accuracy improves\n",
    "                batch_loss = (loss / int(targ.size(0))) # Record a bacth loss\n",
    "                batch_accuracy = accuracy(predictions, targ.to(device))\n",
    "\n",
    "                train_total_loss = train_total_loss + batch_loss\n",
    "                train_total_accuracy = train_total_accuracy + batch_accuracy\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    record_train_accuracy = train_total_accuracy.cpu().detach().numpy()/(batch+1)\n",
    "                    print('Layer {} Epoch {} Batch {} Accuracy {:.4f}. Loss {:.4f}'.format(layers, epoch + 1,\n",
    "                                                                 batch,\n",
    "                                                                 train_total_accuracy.cpu().detach().numpy()/(batch+1),\n",
    "                                                                 train_total_loss.cpu().detach().numpy()/(batch+1)))\n",
    "            # Each epoch is used to calculate the accuracy of the test\n",
    "            print('------------')\n",
    "            model.eval()\n",
    "            test_total_accuracy = 0\n",
    "            for batch, (input_data, target_data) in enumerate(test_loader):\n",
    "                predictions1 = model(input_data.to(device))\n",
    "                batch_accuracy1 = accuracy(predictions1, target_data.to(device))\n",
    "                test_total_accuracy = test_total_accuracy + batch_accuracy1\n",
    "            print('Test : Lay {}, Model {}, Epoch {} Accuracy {:.4f}'.format(layers, key, epoch + 1, test_total_accuracy.cpu().detach().numpy()/(batch+1)))\n",
    "            record_test_accuracy = test_total_accuracy.cpu().detach().numpy()/(batch+1)\n",
    "            end = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "            print('Epoch time : {}mins {}secs'.format(epoch_mins, epoch_secs))\n",
    "            if epoch == num_epochs - 1:\n",
    "                # Write the results of the last round to the file\n",
    "                with open('byr.txt','a') as file:\n",
    "                    file.write('{},{},{:.4f},{:.4f}'.format(key,layers,record_train_accuracy,record_test_accuracy))\n",
    "            print('============')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for GRU\n",
    "\n",
    "- Define the loss function\n",
    "\n",
    "- Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Accuracy 53.1250. Loss 0.0106\n",
      "Epoch 1 Batch 100 Accuracy 51.0675. Loss 0.0115\n",
      "Epoch 1 Batch 200 Accuracy 54.3221. Loss 0.0111\n",
      "Epoch 1 Batch 300 Accuracy 59.4892. Loss 0.0105\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 1 Accuracy 81.8167\n",
      "Epoch time : 2mins 54secs\n",
      "============\n",
      "Epoch 2 Batch 0 Accuracy 89.0625. Loss 0.0049\n",
      "Epoch 2 Batch 100 Accuracy 83.4468. Loss 0.0060\n",
      "Epoch 2 Batch 200 Accuracy 85.1135. Loss 0.0055\n",
      "Epoch 2 Batch 300 Accuracy 85.8544. Loss 0.0053\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 2 Accuracy 87.4512\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 3 Batch 0 Accuracy 90.6250. Loss 0.0032\n",
      "Epoch 3 Batch 100 Accuracy 92.8682. Loss 0.0031\n",
      "Epoch 3 Batch 200 Accuracy 92.6073. Loss 0.0031\n",
      "Epoch 3 Batch 300 Accuracy 92.5405. Loss 0.0031\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 3 Accuracy 88.7396\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 4 Batch 0 Accuracy 92.1875. Loss 0.0031\n",
      "Epoch 4 Batch 100 Accuracy 96.7976. Loss 0.0016\n",
      "Epoch 4 Batch 200 Accuracy 96.5330. Loss 0.0017\n",
      "Epoch 4 Batch 300 Accuracy 96.2832. Loss 0.0018\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 4 Accuracy 88.7540\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 5 Batch 0 Accuracy 98.4375. Loss 0.0008\n",
      "Epoch 5 Batch 100 Accuracy 97.7877. Loss 0.0011\n",
      "Epoch 5 Batch 200 Accuracy 97.9011. Loss 0.0011\n",
      "Epoch 5 Batch 300 Accuracy 97.7523. Loss 0.0011\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 5 Accuracy 87.6039\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 6 Batch 0 Accuracy 98.4375. Loss 0.0004\n",
      "Epoch 6 Batch 100 Accuracy 99.0099. Loss 0.0006\n",
      "Epoch 6 Batch 200 Accuracy 98.8573. Loss 0.0007\n",
      "Epoch 6 Batch 300 Accuracy 98.8943. Loss 0.0006\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 6 Accuracy 88.3863\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 7 Batch 0 Accuracy 100.0000. Loss 0.0002\n",
      "Epoch 7 Batch 100 Accuracy 99.0563. Loss 0.0005\n",
      "Epoch 7 Batch 200 Accuracy 99.0127. Loss 0.0005\n",
      "Epoch 7 Batch 300 Accuracy 98.9566. Loss 0.0006\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 7 Accuracy 88.0451\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 8 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 8 Batch 100 Accuracy 99.2420. Loss 0.0004\n",
      "Epoch 8 Batch 200 Accuracy 99.2149. Loss 0.0004\n",
      "Epoch 8 Batch 300 Accuracy 99.2058. Loss 0.0004\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 8 Accuracy 88.3919\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 9 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 9 Batch 100 Accuracy 99.4585. Loss 0.0003\n",
      "Epoch 9 Batch 200 Accuracy 99.3315. Loss 0.0004\n",
      "Epoch 9 Batch 300 Accuracy 99.2681. Loss 0.0004\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 9 Accuracy 88.3656\n",
      "Epoch time : 2mins 55secs\n",
      "============\n",
      "Epoch 10 Batch 0 Accuracy 100.0000. Loss 0.0002\n",
      "Epoch 10 Batch 100 Accuracy 99.6906. Loss 0.0002\n",
      "Epoch 10 Batch 200 Accuracy 99.6113. Loss 0.0003\n",
      "Epoch 10 Batch 300 Accuracy 99.5743. Loss 0.0003\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 10 Accuracy 88.3840\n",
      "Epoch time : 2mins 55secs\n",
      "============\n",
      "Epoch 11 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 11 Batch 100 Accuracy 99.4585. Loss 0.0003\n",
      "Epoch 11 Batch 200 Accuracy 99.4248. Loss 0.0004\n",
      "Epoch 11 Batch 300 Accuracy 99.3511. Loss 0.0004\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 11 Accuracy 88.2433\n",
      "Epoch time : 2mins 55secs\n",
      "============\n",
      "Epoch 12 Batch 0 Accuracy 98.4375. Loss 0.0002\n",
      "Epoch 12 Batch 100 Accuracy 99.3502. Loss 0.0004\n",
      "Epoch 12 Batch 200 Accuracy 99.3470. Loss 0.0004\n",
      "Epoch 12 Batch 300 Accuracy 99.3355. Loss 0.0004\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 12 Accuracy 88.0059\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 13 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 13 Batch 100 Accuracy 99.6442. Loss 0.0002\n",
      "Epoch 13 Batch 200 Accuracy 99.6346. Loss 0.0002\n",
      "Epoch 13 Batch 300 Accuracy 99.5328. Loss 0.0003\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 13 Accuracy 87.7158\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 14 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 14 Batch 100 Accuracy 99.6132. Loss 0.0003\n",
      "Epoch 14 Batch 200 Accuracy 99.3081. Loss 0.0004\n",
      "Epoch 14 Batch 300 Accuracy 99.2940. Loss 0.0004\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 14 Accuracy 88.2984\n",
      "Epoch time : 2mins 54secs\n",
      "============\n",
      "Epoch 15 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 15 Batch 100 Accuracy 99.6287. Loss 0.0002\n",
      "Epoch 15 Batch 200 Accuracy 99.5958. Loss 0.0002\n",
      "Epoch 15 Batch 300 Accuracy 99.5380. Loss 0.0003\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 15 Accuracy 88.0587\n",
      "Epoch time : 2mins 54secs\n",
      "============\n",
      "Epoch 16 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 16 Batch 100 Accuracy 99.6597. Loss 0.0001\n",
      "Epoch 16 Batch 200 Accuracy 99.6346. Loss 0.0002\n",
      "Epoch 16 Batch 300 Accuracy 99.6418. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 16 Accuracy 88.3344\n",
      "Epoch time : 2mins 55secs\n",
      "============\n",
      "Epoch 17 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 17 Batch 100 Accuracy 99.6597. Loss 0.0002\n",
      "Epoch 17 Batch 200 Accuracy 99.6735. Loss 0.0002\n",
      "Epoch 17 Batch 300 Accuracy 99.6470. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 17 Accuracy 87.8732\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 18 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 18 Batch 100 Accuracy 99.7989. Loss 0.0001\n",
      "Epoch 18 Batch 200 Accuracy 99.4714. Loss 0.0003\n",
      "Epoch 18 Batch 300 Accuracy 99.2058. Loss 0.0005\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 18 Accuracy 87.6175\n",
      "Epoch time : 2mins 54secs\n",
      "============\n",
      "Epoch 19 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 19 Batch 100 Accuracy 99.2110. Loss 0.0004\n",
      "Epoch 19 Batch 200 Accuracy 99.2537. Loss 0.0004\n",
      "Epoch 19 Batch 300 Accuracy 99.2629. Loss 0.0004\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 19 Accuracy 87.8676\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 20 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 20 Batch 100 Accuracy 99.7525. Loss 0.0001\n",
      "Epoch 20 Batch 200 Accuracy 99.7435. Loss 0.0002\n",
      "Epoch 20 Batch 300 Accuracy 99.7353. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 20 Accuracy 88.1314\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 21 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 21 Batch 100 Accuracy 99.7061. Loss 0.0001\n",
      "Epoch 21 Batch 200 Accuracy 99.8057. Loss 0.0001\n",
      "Epoch 21 Batch 300 Accuracy 99.8079. Loss 0.0001\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 21 Accuracy 88.0571\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 22 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 22 Batch 100 Accuracy 99.9381. Loss 0.0000\n",
      "Epoch 22 Batch 200 Accuracy 99.8989. Loss 0.0001\n",
      "Epoch 22 Batch 300 Accuracy 99.8598. Loss 0.0001\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 22 Accuracy 87.9859\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 23 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 23 Batch 100 Accuracy 99.7525. Loss 0.0001\n",
      "Epoch 23 Batch 200 Accuracy 99.7046. Loss 0.0002\n",
      "Epoch 23 Batch 300 Accuracy 99.6418. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 23 Accuracy 87.7909\n",
      "Epoch time : 2mins 54secs\n",
      "============\n",
      "Epoch 24 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 24 Batch 100 Accuracy 99.6442. Loss 0.0002\n",
      "Epoch 24 Batch 200 Accuracy 99.6424. Loss 0.0002\n",
      "Epoch 24 Batch 300 Accuracy 99.6470. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 24 Accuracy 87.7885\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 25 Batch 0 Accuracy 100.0000. Loss 0.0000\n",
      "Epoch 25 Batch 100 Accuracy 99.6906. Loss 0.0002\n",
      "Epoch 25 Batch 200 Accuracy 99.7357. Loss 0.0002\n",
      "Epoch 25 Batch 300 Accuracy 99.6574. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 25 Accuracy 87.9412\n",
      "Epoch time : 2mins 54secs\n",
      "============\n",
      "Epoch 26 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 26 Batch 100 Accuracy 99.7061. Loss 0.0002\n",
      "Epoch 26 Batch 200 Accuracy 99.6424. Loss 0.0002\n",
      "Epoch 26 Batch 300 Accuracy 99.6055. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 26 Accuracy 87.6798\n",
      "Epoch time : 2mins 54secs\n",
      "============\n",
      "Epoch 27 Batch 0 Accuracy 98.4375. Loss 0.0003\n",
      "Epoch 27 Batch 100 Accuracy 99.7834. Loss 0.0001\n",
      "Epoch 27 Batch 200 Accuracy 99.7124. Loss 0.0002\n",
      "Epoch 27 Batch 300 Accuracy 99.6574. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 27 Accuracy 87.7398\n",
      "Epoch time : 2mins 54secs\n",
      "============\n",
      "Epoch 28 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 28 Batch 100 Accuracy 99.7679. Loss 0.0002\n",
      "Epoch 28 Batch 200 Accuracy 99.8057. Loss 0.0001\n",
      "Epoch 28 Batch 300 Accuracy 99.7872. Loss 0.0001\n",
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test : Lay 2, Model GRU, Epoch 28 Accuracy 88.0195\n",
      "Epoch time : 2mins 53secs\n",
      "============\n",
      "Epoch 29 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 29 Batch 100 Accuracy 99.7525. Loss 0.0001\n",
      "Epoch 29 Batch 200 Accuracy 99.7435. Loss 0.0001\n",
      "Epoch 29 Batch 300 Accuracy 99.7664. Loss 0.0001\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 29 Accuracy 87.6455\n",
      "Epoch time : 2mins 56secs\n",
      "============\n",
      "Epoch 30 Batch 0 Accuracy 100.0000. Loss 0.0001\n",
      "Epoch 30 Batch 100 Accuracy 99.7679. Loss 0.0001\n",
      "Epoch 30 Batch 200 Accuracy 99.6580. Loss 0.0002\n",
      "Epoch 30 Batch 300 Accuracy 99.6159. Loss 0.0002\n",
      "------------\n",
      "Test : Lay 2, Model GRU, Epoch 30 Accuracy 86.6081\n",
      "Epoch time : 2mins 54secs\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "vocab_inp_size = len(lang_process.word2index) + 2\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2 \n",
    "num_layers = [2]\n",
    "\n",
    "for layers in num_layers:\n",
    "    modelGRU = EmotionGRU(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size, layers).to(device)\n",
    "    models = {}\n",
    "    \n",
    "    models['GRU'] = modelGRU\n",
    "    for key, model in models.items():\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "        num_epochs = 30\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            start = time.time()\n",
    "\n",
    "            train_total_loss = 0 \n",
    "            train_total_accuracy = 0 \n",
    "\n",
    "            for batch, (inp, targ) in enumerate(train_loader):\n",
    "                predictions = model(inp.to(device))  \n",
    "                loss = criterion(predictions, targ.to(device))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss = (loss / int(targ.size(0)))  \n",
    "                batch_accuracy = accuracy(predictions, targ.to(device))\n",
    "\n",
    "                train_total_loss = train_total_loss + batch_loss\n",
    "                train_total_accuracy = train_total_accuracy + batch_accuracy\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    record_train_accuracy = train_total_accuracy.cpu().detach().numpy()/(batch+1)\n",
    "                    print('Epoch {} Batch {} Accuracy {:.4f}. Loss {:.4f}'.format(epoch + 1,\n",
    "                                                                 batch,\n",
    "                                                                 train_total_accuracy.cpu().detach().numpy()/(batch+1),\n",
    "                                                                 train_total_loss.cpu().detach().numpy()/(batch+1)))\n",
    "            \n",
    "            print('------------')\n",
    "            model.eval()\n",
    "            test_total_accuracy = 0\n",
    "            for batch, (input_data, target_data) in enumerate(test_loader):\n",
    "                predictions = model(input_data.to(device))\n",
    "                batch_accuracy = accuracy(predictions, target_data.to(device))\n",
    "                test_total_accuracy = test_total_accuracy + batch_accuracy\n",
    "            print('Test : Lay {}, Model {}, Epoch {} Accuracy {:.4f}'.format(layers, key, epoch + 1, test_total_accuracy.cpu().detach().numpy()/(batch+1)))\n",
    "            record_test_accuracy = test_total_accuracy.cpu().detach().numpy()/(batch+1)\n",
    "            end = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "            print('Epoch time : {}mins {}secs'.format(epoch_mins, epoch_secs))\n",
    "            if epoch == num_epochs - 1:\n",
    "                with open('byr.txt','a') as file:\n",
    "                    file.write('{},{},{:.4f},{:.4f}'.format(key,layers,record_train_accuracy,record_test_accuracy))\n",
    "            print('============')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training for Simple RNN\n",
    "\n",
    "- Define the loss function\n",
    "\n",
    "- Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Accuracy 37.5000. Loss 0.0127\n",
      "Epoch 1 Batch 100 Accuracy 49.7679. Loss 0.0109\n",
      "Epoch 1 Batch 200 Accuracy 50.3654. Loss 0.0109\n",
      "Epoch 1 Batch 300 Accuracy 50.3426. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 1 Accuracy 49.9544\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 2 Batch 0 Accuracy 40.6250. Loss 0.0108\n",
      "Epoch 2 Batch 100 Accuracy 48.9790. Loss 0.0108\n",
      "Epoch 2 Batch 200 Accuracy 48.8495. Loss 0.0108\n",
      "Epoch 2 Batch 300 Accuracy 49.2577. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 2 Accuracy 49.9536\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 3 Batch 0 Accuracy 50.0000. Loss 0.0108\n",
      "Epoch 3 Batch 100 Accuracy 49.2420. Loss 0.0108\n",
      "Epoch 3 Batch 200 Accuracy 49.5569. Loss 0.0108\n",
      "Epoch 3 Batch 300 Accuracy 49.7301. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 3 Accuracy 50.0296\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 4 Batch 0 Accuracy 48.4375. Loss 0.0108\n",
      "Epoch 4 Batch 100 Accuracy 50.0464. Loss 0.0108\n",
      "Epoch 4 Batch 200 Accuracy 49.3004. Loss 0.0108\n",
      "Epoch 4 Batch 300 Accuracy 49.0449. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 4 Accuracy 50.0160\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 5 Batch 0 Accuracy 53.1250. Loss 0.0108\n",
      "Epoch 5 Batch 100 Accuracy 50.2785. Loss 0.0108\n",
      "Epoch 5 Batch 200 Accuracy 50.1166. Loss 0.0108\n",
      "Epoch 5 Batch 300 Accuracy 50.0831. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 5 Accuracy 49.9664\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 6 Batch 0 Accuracy 50.0000. Loss 0.0108\n",
      "Epoch 6 Batch 100 Accuracy 50.6188. Loss 0.0108\n",
      "Epoch 6 Batch 200 Accuracy 50.2177. Loss 0.0108\n",
      "Epoch 6 Batch 300 Accuracy 50.0415. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 6 Accuracy 49.9576\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 7 Batch 0 Accuracy 43.7500. Loss 0.0109\n",
      "Epoch 7 Batch 100 Accuracy 49.5978. Loss 0.0108\n",
      "Epoch 7 Batch 200 Accuracy 50.0622. Loss 0.0108\n",
      "Epoch 7 Batch 300 Accuracy 50.0208. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 7 Accuracy 49.9632\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 8 Batch 0 Accuracy 48.4375. Loss 0.0109\n",
      "Epoch 8 Batch 100 Accuracy 49.6751. Loss 0.0108\n",
      "Epoch 8 Batch 200 Accuracy 49.6346. Loss 0.0108\n",
      "Epoch 8 Batch 300 Accuracy 49.8702. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 8 Accuracy 50.0000\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 9 Batch 0 Accuracy 50.0000. Loss 0.0109\n",
      "Epoch 9 Batch 100 Accuracy 50.9746. Loss 0.0108\n",
      "Epoch 9 Batch 200 Accuracy 50.9173. Loss 0.0108\n",
      "Epoch 9 Batch 300 Accuracy 50.7371. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 9 Accuracy 50.0160\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 10 Batch 0 Accuracy 48.4375. Loss 0.0108\n",
      "Epoch 10 Batch 100 Accuracy 50.3403. Loss 0.0108\n",
      "Epoch 10 Batch 200 Accuracy 50.3498. Loss 0.0108\n",
      "Epoch 10 Batch 300 Accuracy 50.2751. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 10 Accuracy 49.9680\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 11 Batch 0 Accuracy 39.0625. Loss 0.0109\n",
      "Epoch 11 Batch 100 Accuracy 49.5978. Loss 0.0108\n",
      "Epoch 11 Batch 200 Accuracy 49.7512. Loss 0.0108\n",
      "Epoch 11 Batch 300 Accuracy 49.9740. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 11 Accuracy 50.0120\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 12 Batch 0 Accuracy 57.8125. Loss 0.0108\n",
      "Epoch 12 Batch 100 Accuracy 50.8354. Loss 0.0108\n",
      "Epoch 12 Batch 200 Accuracy 50.0155. Loss 0.0108\n",
      "Epoch 12 Batch 300 Accuracy 49.9377. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 12 Accuracy 50.0344\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 13 Batch 0 Accuracy 46.8750. Loss 0.0108\n",
      "Epoch 13 Batch 100 Accuracy 50.4332. Loss 0.0108\n",
      "Epoch 13 Batch 200 Accuracy 50.3576. Loss 0.0108\n",
      "Epoch 13 Batch 300 Accuracy 50.1505. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 13 Accuracy 50.0136\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 14 Batch 0 Accuracy 56.2500. Loss 0.0108\n",
      "Epoch 14 Batch 100 Accuracy 50.4796. Loss 0.0108\n",
      "Epoch 14 Batch 200 Accuracy 50.0389. Loss 0.0108\n",
      "Epoch 14 Batch 300 Accuracy 50.1505. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 14 Accuracy 50.0240\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 15 Batch 0 Accuracy 43.7500. Loss 0.0110\n",
      "Epoch 15 Batch 100 Accuracy 49.1646. Loss 0.0108\n",
      "Epoch 15 Batch 200 Accuracy 49.8912. Loss 0.0108\n",
      "Epoch 15 Batch 300 Accuracy 49.3511. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 15 Accuracy 49.9576\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 16 Batch 0 Accuracy 50.0000. Loss 0.0108\n",
      "Epoch 16 Batch 100 Accuracy 50.5879. Loss 0.0108\n",
      "Epoch 16 Batch 200 Accuracy 50.4353. Loss 0.0108\n",
      "Epoch 16 Batch 300 Accuracy 50.2751. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 16 Accuracy 49.9720\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 17 Batch 0 Accuracy 50.0000. Loss 0.0108\n",
      "Epoch 17 Batch 100 Accuracy 49.2265. Loss 0.0108\n",
      "Epoch 17 Batch 200 Accuracy 50.3265. Loss 0.0108\n",
      "Epoch 17 Batch 300 Accuracy 50.2855. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 17 Accuracy 49.9784\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 18 Batch 0 Accuracy 40.6250. Loss 0.0108\n",
      "Epoch 18 Batch 100 Accuracy 49.9072. Loss 0.0108\n",
      "Epoch 18 Batch 200 Accuracy 49.2848. Loss 0.0108\n",
      "Epoch 18 Batch 300 Accuracy 49.4498. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 18 Accuracy 50.0136\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 19 Batch 0 Accuracy 43.7500. Loss 0.0109\n",
      "Epoch 19 Batch 100 Accuracy 49.8762. Loss 0.0108\n",
      "Epoch 19 Batch 200 Accuracy 49.6968. Loss 0.0108\n",
      "Epoch 19 Batch 300 Accuracy 50.0104. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 19 Accuracy 50.0024\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 20 Batch 0 Accuracy 51.5625. Loss 0.0108\n",
      "Epoch 20 Batch 100 Accuracy 49.8453. Loss 0.0108\n",
      "Epoch 20 Batch 200 Accuracy 50.0389. Loss 0.0108\n",
      "Epoch 20 Batch 300 Accuracy 50.0779. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 20 Accuracy 49.9552\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 21 Batch 0 Accuracy 43.7500. Loss 0.0109\n",
      "Epoch 21 Batch 100 Accuracy 50.3868. Loss 0.0108\n",
      "Epoch 21 Batch 200 Accuracy 50.2410. Loss 0.0108\n",
      "Epoch 21 Batch 300 Accuracy 50.4257. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 21 Accuracy 50.0168\n",
      "Epoch time : 0mins 46secs\n",
      "============\n",
      "Epoch 22 Batch 0 Accuracy 48.4375. Loss 0.0108\n",
      "Epoch 22 Batch 100 Accuracy 51.0210. Loss 0.0108\n",
      "Epoch 22 Batch 200 Accuracy 50.6141. Loss 0.0108\n",
      "Epoch 22 Batch 300 Accuracy 50.2128. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 22 Accuracy 49.9728\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 23 Batch 0 Accuracy 53.1250. Loss 0.0108\n",
      "Epoch 23 Batch 100 Accuracy 50.5879. Loss 0.0108\n",
      "Epoch 23 Batch 200 Accuracy 50.6996. Loss 0.0108\n",
      "Epoch 23 Batch 300 Accuracy 50.6333. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 23 Accuracy 50.0120\n",
      "Epoch time : 0mins 46secs\n",
      "============\n",
      "Epoch 24 Batch 0 Accuracy 43.7500. Loss 0.0109\n",
      "Epoch 24 Batch 100 Accuracy 49.6597. Loss 0.0108\n",
      "Epoch 24 Batch 200 Accuracy 49.7357. Loss 0.0108\n",
      "Epoch 24 Batch 300 Accuracy 49.7093. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 24 Accuracy 50.0112\n",
      "Epoch time : 0mins 46secs\n",
      "============\n",
      "Epoch 25 Batch 0 Accuracy 57.8125. Loss 0.0108\n",
      "Epoch 25 Batch 100 Accuracy 50.1702. Loss 0.0108\n",
      "Epoch 25 Batch 200 Accuracy 50.6685. Loss 0.0108\n",
      "Epoch 25 Batch 300 Accuracy 50.5191. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 25 Accuracy 49.9592\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 26 Batch 0 Accuracy 54.6875. Loss 0.0108\n",
      "Epoch 26 Batch 100 Accuracy 49.0873. Loss 0.0108\n",
      "Epoch 26 Batch 200 Accuracy 49.6424. Loss 0.0108\n",
      "Epoch 26 Batch 300 Accuracy 49.9948. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 26 Accuracy 49.9576\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 27 Batch 0 Accuracy 43.7500. Loss 0.0109\n",
      "Epoch 27 Batch 100 Accuracy 49.5050. Loss 0.0108\n",
      "Epoch 27 Batch 200 Accuracy 49.7512. Loss 0.0108\n",
      "Epoch 27 Batch 300 Accuracy 49.9377. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 27 Accuracy 49.9600\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 28 Batch 0 Accuracy 40.6250. Loss 0.0109\n",
      "Epoch 28 Batch 100 Accuracy 49.7215. Loss 0.0108\n",
      "Epoch 28 Batch 200 Accuracy 49.8445. Loss 0.0108\n",
      "Epoch 28 Batch 300 Accuracy 49.7820. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 28 Accuracy 50.0000\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 29 Batch 0 Accuracy 39.0625. Loss 0.0109\n",
      "Epoch 29 Batch 100 Accuracy 49.6751. Loss 0.0108\n",
      "Epoch 29 Batch 200 Accuracy 49.4947. Loss 0.0108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 300 Accuracy 49.6937. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 29 Accuracy 50.0104\n",
      "Epoch time : 0mins 45secs\n",
      "============\n",
      "Epoch 30 Batch 0 Accuracy 39.0625. Loss 0.0108\n",
      "Epoch 30 Batch 100 Accuracy 49.8608. Loss 0.0108\n",
      "Epoch 30 Batch 200 Accuracy 50.0389. Loss 0.0108\n",
      "Epoch 30 Batch 300 Accuracy 50.0052. Loss 0.0108\n",
      "------------\n",
      "Test : Model RNN, Epoch 30 Accuracy 50.0144\n",
      "Epoch time : 0mins 45secs\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "vocab_inp_size = len(lang_process.word2index)+2\n",
    "embedding_dim = 256\n",
    "hidden_units = 512\n",
    "target_size = 2 \n",
    "num_layers = [1]\n",
    "\n",
    "for layers in num_layers:\n",
    "    modelRNN = EmotionRNN(vocab_inp_size, embedding_dim, hidden_units, MINIBATCH_SIZE, target_size).to(device)\n",
    "    models = {}\n",
    "    \n",
    "    models['RNN'] = modelRNN\n",
    "    for key, model in models.items():\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "        num_epochs = 30\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            start = time.time()\n",
    "\n",
    "            train_total_loss = 0 \n",
    "            train_total_accuracy = 0 \n",
    "\n",
    "            ### Training\n",
    "            for batch, (inp, targ) in enumerate(train_loader):\n",
    "                predictions = model(inp.to(device))  \n",
    "                loss = criterion(predictions, targ.to(device))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss = (loss / int(targ.size(0)))   \n",
    "                batch_accuracy = accuracy(predictions, targ.to(device))\n",
    "\n",
    "                train_total_loss = train_total_loss + batch_loss\n",
    "                train_total_accuracy = train_total_accuracy + batch_accuracy\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    record_train_accuracy = train_total_accuracy.cpu().detach().numpy()/(batch+1)\n",
    "                    print('Epoch {} Batch {} Accuracy {:.4f}. Loss {:.4f}'.format(epoch + 1,\n",
    "                                                                 batch,\n",
    "                                                                 train_total_accuracy.cpu().detach().numpy()/(batch+1),\n",
    "                                                                 train_total_loss.cpu().detach().numpy()/(batch+1)))\n",
    "            \n",
    "            print('------------')\n",
    "            model.eval()\n",
    "            test_total_accuracy = 0\n",
    "            for batch, (input_data, target_data) in enumerate(test_loader):\n",
    "                predictions = model(input_data.to(device))\n",
    "                batch_accuracy = accuracy(predictions, target_data.to(device))\n",
    "                test_total_accuracy = test_total_accuracy + batch_accuracy\n",
    "            print('Test : Model {}, Epoch {} Accuracy {:.4f}'.format(key, epoch + 1, test_total_accuracy.cpu().detach().numpy()/(batch+1)))\n",
    "            record_test_accuracy = test_total_accuracy.cpu().detach().numpy()/(batch+1)\n",
    "            end = time.time()\n",
    "            epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "            print('Epoch time : {}mins {}secs'.format(epoch_mins, epoch_secs))\n",
    "            if epoch == num_epochs - 1:\n",
    "                with open('byr.txt','a') as file:\n",
    "                    file.write('{},{},{:.4f},{:.4f}'.format(key,layers,record_train_accuracy,record_test_accuracy))\n",
    "            print('============')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
